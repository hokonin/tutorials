{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "nmt_with_attention.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "fj7J-Fmdea9i"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fj7J-Fmdea9i",
        "colab_type": "text"
      },
      "source": [
        "##### Copyright 2019 The TensorFlow Authors."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Vg0-zuE9ef5L",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "AOpGoE2T-YXS"
      },
      "source": [
        "# [Neural Machine Translation with Attention](https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KJ760Y73exk9",
        "colab_type": "text"
      },
      "source": [
        "<table class=\"tfo-notebook-buttons\" align=\"left\">\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://www.tensorflow.org/beta/tutorials/text/nmt_with_attention\">\n",
        "    <img src=\"https://www.tensorflow.org/images/tf_logo_32px.png\" />\n",
        "    View on TensorFlow.org</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />\n",
        "    Run in Google Colab</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a target=\"_blank\" href=\"https://github.com/tensorflow/docs/blob/master/site/en/r2/tutorials/text/nmt_with_attention.ipynb\">\n",
        "    <img src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />\n",
        "    View source on GitHub</a>\n",
        "  </td>\n",
        "  <td>\n",
        "    <a href=\"https://storage.googleapis.com/tensorflow_docs/docs/site/en/r2/tutorials/text/nmt_with_attention.ipynb\"><img src=\"https://www.tensorflow.org/images/download_logo_32px.png\" />Download notebook</a>\n",
        "  </td>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "CiwtNgENbx2g"
      },
      "source": [
        "This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). This is an advanced example that assumes some knowledge of sequence to sequence models.\n",
        "\n",
        "> このノートブックは [tf.keras](https://www.tensorflow.org/programmers_guide/keras) と [eager exicution](https://www.tensorflow.org/programmers_guide/eager) を使用してスペイン語を英語に翻訳するシーケンス to シーケンス（seq2seq）モデルを学習します。これはシーケンス to シーケンスモデルに関する知識を前提とした高度な例です。\n",
        "\n",
        "After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"* , and return the English translation: *\"are you still at home?\"*\n",
        "\n",
        "> このノートブックでモデルを訓練した後は *\"¿todavia estan en casa?\"* などのスペイン語の文章を入力して、英語の翻訳 *\"are you still at home?\"* を返すことができます。\n",
        "\n",
        "The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n",
        "\n",
        "> 翻訳の質は例題としては妥当ですが、生成された Attention プロットはおそらくもっと興味深いものです。これは翻訳中に入力文のどの部分にモデルが注目したかを示します。\n",
        "\n",
        "<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n",
        "\n",
        "Note: This example takes approximately 10 mintues to run on a single P100 GPU.\n",
        "\n",
        "> 注：この例では1台のP100 GPUで実行するのに約10分かかります。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "tnxXKDjq3jEL",
        "outputId": "a9366bcb-9d7c-489a-eb40-273928858776",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        }
      },
      "source": [
        "from __future__ import absolute_import\n",
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "from __future__ import unicode_literals\n",
        "\n",
        "# Import TensorFlow >= 1.10 and enable eager execution\n",
        "!pip install tensorflow-gpu==2.0.0-beta1\n",
        "import tensorflow as tf\n",
        "# tf.enable_eager_execution()\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "import unicodedata\n",
        "import re\n",
        "import numpy as np\n",
        "import os\n",
        "import io\n",
        "import time\n",
        "\n",
        "print(tf.__version__)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting tensorflow-gpu==2.0.0-beta1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/2b/53/e18c5e7a2263d3581a979645a185804782e59b8e13f42b9c3c3cfb5bb503/tensorflow_gpu-2.0.0b1-cp36-cp36m-manylinux1_x86_64.whl (348.9MB)\n",
            "\u001b[K     |████████████████████████████████| 348.9MB 91kB/s \n",
            "\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n",
            "Collecting tb-nightly<1.14.0a20190604,>=1.14.0a20190603 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a4/96/571b875cd81dda9d5dfa1422a4f9d749e67c0a8d4f4f0b33a4e5f5f35e27/tb_nightly-1.14.0a20190603-py3-none-any.whl (3.1MB)\n",
            "\u001b[K     |████████████████████████████████| 3.1MB 41.4MB/s \n",
            "\u001b[?25hRequirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n",
            "Collecting tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 (from tensorflow-gpu==2.0.0-beta1)\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/32/dd/99c47dd007dcf10d63fd895611b063732646f23059c618a373e85019eb0e/tf_estimator_nightly-1.14.0.dev2019060501-py2.py3-none-any.whl (496kB)\n",
            "\u001b[K     |████████████████████████████████| 501kB 50.7MB/s \n",
            "\u001b[?25hRequirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n",
            "Installing collected packages: tb-nightly, tf-estimator-nightly, tensorflow-gpu\n",
            "Successfully installed tb-nightly-1.14.0a20190603 tensorflow-gpu-2.0.0b1 tf-estimator-nightly-1.14.0.dev2019060501\n",
            "2.0.0-beta1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0fzSjPE_vT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from typing import List, Dict, Sequence, Tuple, Iterable"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "wfodePkj3jEa"
      },
      "source": [
        "## Download and prepare the dataset\n",
        "\n",
        "We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n",
        "\n",
        "> http://www.manythings.org/anki/ で提供されている言語データセットを使用します。このデータセットには次の形式の言語翻訳ペアが含まれています。\n",
        "\n",
        "```\n",
        "May I borrow this book?\t¿Puedo tomar prestado este libro?\n",
        "```\n",
        "\n",
        "There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n",
        "\n",
        "> さまざまな言語がありますが、ここでは英語 - スペイン語のデータセットを使用します。便宜上、このデータセットのコピーをGoogle Cloudでホストしていますが、自分のコピーをダウンロードすることもできます。データセットをダウンロードした後、データを準備するための手順は次のとおりです。\n",
        "\n",
        "1. Add a *start* and *end* token to each sentence.<br />各文に開始トークンと終了トークンを追加します。\n",
        "2. Clean the sentences by removing special characters.<br />特殊文字を削除して文章をきれいにします。\n",
        "3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).<br />単語インデクスと逆単語インデクスを作成します（単語→id と id→単語 のマッピング辞書）。\n",
        "4. Pad each sentence to a maximum length.<br />各文を最大長まで埋めます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kRVATYOgJs1b",
        "outputId": "2664db1b-b849-44b7-8bac-40035f2fd7dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# Download the file\n",
        "path_to_zip = tf.keras.utils.get_file(\n",
        "    'spa-eng.zip',\n",
        "    origin='http://storage.googleapis.com/download.tensorflow.org'\n",
        "        '/data/spa-eng.zip',\n",
        "    extract=True)\n",
        "\n",
        "path_to_file = os.path.dirname(path_to_zip) + '/spa-eng/spa.txt'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from http://storage.googleapis.com/download.tensorflow.org/data/spa-eng.zip\n",
            "2646016/2638744 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "rd0jw-eC3jEh",
        "colab": {}
      },
      "source": [
        "# Converts the unicode file to ascii\n",
        "def unicode_to_ascii(s: str) -> str:\n",
        "    return ''.join(\n",
        "        c for c in unicodedata.normalize('NFD', s)\n",
        "        if unicodedata.category(c) != 'Mn')\n",
        "\n",
        "def preprocess_sentence(w: str) -> str:\n",
        "    w = unicode_to_ascii(w.lower().strip())\n",
        "    \n",
        "    # creating a space between a word and the punctuation following it\n",
        "    # eg: \"he is a boy.\" => \"he is a boy .\" \n",
        "    # Reference:- https://stackoverflow.com/questions/3645931\n",
        "    #     /python-padding-punctuation-with-white-spaces-keeping-punctuation\n",
        "    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n",
        "    w = re.sub(r'[\" \"]+', \" \", w)\n",
        "    \n",
        "    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n",
        "    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n",
        "    \n",
        "    w = w.rstrip().strip()\n",
        "    \n",
        "    # adding a start and an end token to the sentence\n",
        "    # so that the model know when to start and stop predicting.\n",
        "    w = '<start> ' + w + ' <end>'\n",
        "    return w"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nZ-OvGrnJHiA",
        "colab_type": "code",
        "outputId": "87c4577d-df22-4d18-8237-8a7c6c17014a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "en_sentence = 'May I borrow this book?'\n",
        "sp_sentence = '¿Puedo tomar prestado este libro?'\n",
        "print(preprocess_sentence(en_sentence))\n",
        "print(preprocess_sentence(sp_sentence).encode('utf-8'))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> may i borrow this book ? <end>\n",
            "b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "OHn4Dct23jEm",
        "colab": {}
      },
      "source": [
        "# 1. Remove the accents\n",
        "# 2. Clean the sentences\n",
        "# 3. Return word pairs in the format: zip[ENGLISH(Tuple), SPANISH(Tuple)]\n",
        "def create_dataset(path: str, num_examples: int) -> zip:\n",
        "    lines = open(path, encoding='UTF-8').read().strip().split('\\n')\n",
        "    \n",
        "    word_pairs = [\n",
        "        [preprocess_sentence(w) for w in l.split('\\t')]\n",
        "        for l in lines[:num_examples]\n",
        "    ]\n",
        "    \n",
        "    return zip(*word_pairs)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ecBRQmSiMswx",
        "colab_type": "code",
        "outputId": "5cfd13f9-d2b1-4434-80d3-0922c4faee3b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 71
        }
      },
      "source": [
        "en, sp = create_dataset(path_to_file, None)\n",
        "print(en[-1])\n",
        "print(sp[-1])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<start> if you want to sound like a native speaker , you must be willing to practice saying the same sentence over and over in the same way that banjo players practice the same phrase over and over until they can play it correctly and at the desired tempo . <end>\n",
            "<start> si quieres sonar como un hablante nativo , debes estar dispuesto a practicar diciendo la misma frase una y otra vez de la misma manera en que un musico de banjo practica el mismo fraseo una y otra vez hasta que lo puedan tocar correctamente y en el tiempo esperado . <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YW021kdSOZnN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def max_length(tensor):\n",
        "    return max(len(t) for t in tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fhHZb8PZOeCb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def tokenize(\n",
        "    lang: Sequence[str]\n",
        ") -> Tuple[np.ndarray, tf.keras.preprocessing.text.Tokenizer]:\n",
        "    \n",
        "    lang_tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
        "    lang_tokenizer.fit_on_texts(lang)\n",
        "\n",
        "    tensor = lang_tokenizer.texts_to_sequences(lang)\n",
        "    tensor = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        tensor,\n",
        "        padding='post')\n",
        "\n",
        "    return tensor, lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YRknl_MdO4D-",
        "colab_type": "text"
      },
      "source": [
        "> [Tokenizer](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/text/Tokenizer)\n",
        "> <br />\n",
        "> [pad_sequences](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "eAY9k49G3jE_",
        "colab": {}
      },
      "source": [
        "def load_dataset(path: str, num_examples: int = None):\n",
        "    # creating cleaned input, output pairs\n",
        "    targ_lang, inp_lang = create_dataset(path, num_examples)\n",
        "\n",
        "    input_tensor, inp_lang_tokenizer = tokenize(inp_lang)\n",
        "    target_tensor, targ_lang_tokenizer = tokenize(targ_lang)\n",
        "\n",
        "    return input_tensor, target_tensor, inp_lang_tokenizer, targ_lang_tokenizer"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "GOi42V79Ydlr"
      },
      "source": [
        "### Limit the size of the dataset to experiment faster (optional)\n",
        "\n",
        "Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):\n",
        "\n",
        "> 10万文を超える完全なデータセットのトレーニングには長い時間がかかります。より早く訓練するために、データセットのサイズを3万文に制限することができます（もちろん少ないデータでは翻訳品質が低下します）。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cnxC7q-j3jFD",
        "colab": {}
      },
      "source": [
        "# Try experimenting with the size of that dataset\n",
        "num_examples = None  # 30000\n",
        "input_tensor, target_tensor, inp_lang, targ_lang = load_dataset(\n",
        "    path_to_file,\n",
        "    num_examples)\n",
        "\n",
        "# Calculate max_length of the input tensors and the target tensors\n",
        "max_length_inp = max_length(input_tensor)\n",
        "max_length_targ = max_length(target_tensor)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4QILQkOs3jFG",
        "outputId": "e614be39-6cf6-4e5a-a87e-24b1dc1f2894",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Creating training and validation sets using an 80-20 split\n",
        "(input_tensor_train, input_tensor_val,\n",
        " target_tensor_train, target_tensor_val) = train_test_split(\n",
        "    input_tensor,\n",
        "    target_tensor,\n",
        "    test_size=0.2)\n",
        "\n",
        "# Show length\n",
        "(len(input_tensor_train), len(target_tensor_train),\n",
        " len(input_tensor_val), len(target_tensor_val))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(95171, 95171, 23793, 23793)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8aasjzLnUGfl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def convert(lang, tensor):\n",
        "    for t in tensor:\n",
        "        if t != 0:\n",
        "            print(\"%d ----> %s\" % (t, lang.index_word[t]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rFaZEKk6UK4l",
        "colab_type": "code",
        "outputId": "515cb4bd-6af7-4016-e6dd-a43c62df94cd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "print(\"Input Language; index to word mapping\")\n",
        "convert(inp_lang, input_tensor_train[0])\n",
        "print()\n",
        "print(\"Target Language; index to word mapping\")\n",
        "convert(targ_lang, target_tensor_train[0])"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input Language; index to word mapping\n",
            "1 ----> <start>\n",
            "2188 ----> ponte\n",
            "6 ----> el\n",
            "682 ----> sombrero\n",
            "3 ----> .\n",
            "2 ----> <end>\n",
            "\n",
            "Target Language; index to word mapping\n",
            "1 ----> <start>\n",
            "175 ----> put\n",
            "35 ----> your\n",
            "698 ----> hat\n",
            "36 ----> on\n",
            "3 ----> .\n",
            "2 ----> <end>\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rgCLkfv5uO3d"
      },
      "source": [
        "### Create a tf.data dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "TqHsArVZ3jFS",
        "colab": {}
      },
      "source": [
        "BUFFER_SIZE = len(input_tensor_train)\n",
        "BATCH_SIZE = 64\n",
        "steps_per_epoch = BUFFER_SIZE // BATCH_SIZE\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "vocab_inp_size = len(inp_lang.word_index) + 1\n",
        "vocab_tar_size = len(targ_lang.word_index) + 1\n",
        "\n",
        "dataset = tf.data.Dataset.from_tensor_slices(\n",
        "    (input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n",
        "dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SOudl4Vq2I8u",
        "colab_type": "text"
      },
      "source": [
        "> [from_tensor_slices](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x6SAZjy9UhtQ",
        "colab_type": "code",
        "outputId": "f2385886-9817-460e-a47b-0dd2274c8e94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "example_input_batch, example_target_batch = next(iter(dataset))\n",
        "example_input_batch.shape, example_target_batch.shape"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(TensorShape([64, 53]), TensorShape([64, 51]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m7sIs-GxNb3r",
        "colab_type": "text"
      },
      "source": [
        "## Write the encoder and decoder model\n",
        "\n",
        "Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input word is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n",
        "\n",
        "> ここでは TensorFlow [Neural Machine Translation(seq2seq) チュートリアル](https://github.com/tensorflow/nmt)で読むことができる、Attention付きエンコーダー - デコーダーモデルを実装します。この例ではより新しい一連の API を使用しています。このノートブックは seq2seq チュートリアルから [Attention 計算式](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism)を実装します。次の図は、各入力単語に Attention メカニズムによって重みが割り当てられていることを示しています。このメカニズムは文章内の次の単語を予測するためにデコーダーによって使用されます。\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TNfHIF71ulLu"
      },
      "source": [
        "The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n",
        "\n",
        "> 入力はエンコーダーモデルを通過します。これにより形状が  *(batch_size, max_length, hidden_size)* のエンコーダー出力と、形状が *(batch_size, hidden_size)* のエンコーダー隠れ状態が得られます。\n",
        "\n",
        "Here are the equations that are implemented:\n",
        "\n",
        "> これが実装される計算式です：\n",
        "\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n",
        "<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n",
        "\n",
        "We're using [*Bahdanau attention*](https://arxiv.org/pdf/1409.0473.pdf). Lets decide on notation before writing the simplified form:\n",
        "\n",
        "> 私たちは [*Bahdanau attention*](https://arxiv.org/pdf/1409.0473.pdf) を使います。簡略形で書く前に表記法を決めましょう。\n",
        "\n",
        "* FC = Fully connected (dense) layer\n",
        "* EO = Encoder output\n",
        "* H = hidden state\n",
        "* X = input to the decoder\n",
        "\n",
        "And the pseudo-code:\n",
        "\n",
        ">そして擬似コードです。\n",
        "\n",
        "* `score = FC(tanh(FC(EO) + FC(H)))`<br />\n",
        "> メモ：全結合層は入力と内部の重みの行列積を計算するレイヤーのため、計算式中で $W_1$ や $v_a$ などの重みとの積の部分は `FC` により表現できる。ここで $h_t$ = `H`、$\\bar{h}_s$ = `EO` 。\n",
        "* `attention weights = softmax(score, axis = 1)`<br />\n",
        "Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.<br />\n",
        "Softmax はデフォルトでは最後の軸に適用されますが、ここではスコアの形状が *(batch_size, max_length, 1)* であるため、*1番目の軸* に適用します。 `Max_length` は私達の入力の長さです。各入力に重みを割り当てようとしているので softmax をその軸に適用する必要があります。\n",
        "* `context vector = sum(attention weights * EO, axis = 1)`<br />\n",
        "Same reason as above for choosing axis as 1.<br />1番目の軸を選択したのは上と同じ理由です。\n",
        "* `embedding output` = The input to the decoder X is passed through an embedding layer.<br />デコーダーへの入力 X は埋め込み層を通過します。\n",
        "* `merged vector = concat(embedding output, context vector)`\n",
        "* This merged vector is then given to the GRU<br />このマージされたベクトルは GRU に渡されます\n",
        "  \n",
        "The shapes of all the vectors at each step have been specified in the comments in the code:\n",
        "\n",
        "> 各ステップのすべてのベクトルの形状はコード内のコメントで指定されています。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "nZ2rI24i3jFg",
        "colab": {}
      },
      "source": [
        "class Encoder(tf.keras.Model):\n",
        "    def __init__(\n",
        "            self,\n",
        "            vocab_size: int,  # 語彙数\n",
        "            embedding_dim: int,  # 埋め込みレイヤーで出力する密ベクトルの次元数\n",
        "            enc_units: int,  # Encoder(のGRU）の出力次元数\n",
        "            batch_sz: int):  # バッチサイズ\n",
        "\n",
        "        super(Encoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.enc_units = enc_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.enc_units,\n",
        "            return_sequences=True,\n",
        "            return_state=True,\n",
        "            recurrent_initializer='glorot_uniform')\n",
        "\n",
        "    # 入力 x は int の ndarray を convert_to_tensor() したもの\n",
        "    # 型としては tensorflow.python.framework.ops.EagerTensor …\n",
        "    # hidden は初回は initialze_hidden_state で返却したものが渡ってくる\n",
        "    def call(self, x, hidden):\n",
        "        x = self.embedding(x)\n",
        "        output, state = self.gru(x, initial_state=hidden)\n",
        "        return output, state\n",
        "    \n",
        "    def initialize_hidden_state(self):\n",
        "        return tf.zeros((self.batch_sz, self.enc_units))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZeCPb-zC2Rt",
        "colab_type": "text"
      },
      "source": [
        "> [Embedding](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding)\n",
        "> <br />\n",
        "> <br />Embedding レイヤーは入力の整数値（単語ID）を固定長の密ベクトルに変換する。第1引数 input_dim は語彙数。第2引数 output_dim は出力する密ベクトルの次元数。\n",
        "\n",
        "> [GRU](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU)\n",
        "> <br />\n",
        "> <br />※GPUが使われる条件について抜粋※\n",
        "> <br />利用可能なランタイムハードウェアと制約に基づいて、このレイヤーはパフォーマンス最大化のため異なる実装（cuDNNベースまたは純粋なTensorFlow）を選択します。 GPUが利用可能で、レイヤーに対するすべての引数がCuDNNカーネルの要件を満たす場合（詳細は下記を参照）、高速なcuDNN実装を使用します。\n",
        "> <br />\n",
        "> <br />cuDNN実装を使用するための要件は次のとおりです。\n",
        "> <br />\n",
        "> 1. activation == 'tanh'\n",
        "> 1. recurrent_activation == 'sigmoid'\n",
        "> 1. recurrent_dropout == 0\n",
        "> 1. unroll is False\n",
        "> 1. use_bias is True\n",
        "> 1. reset_after is True\n",
        "> 1. 入力がマスクされていないか、厳密に右詰めされている\n",
        "> <br />メモ：1 ～ 6 は既定値のため、この入力値の条件さえ満たせば GPU が使われる"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fHopunMuVtIl",
        "colab_type": "code",
        "outputId": "849257d9-c57b-4660-c97a-ea5c589a2dbe",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "# sample input\n",
        "sample_hidden = encoder.initialize_hidden_state()\n",
        "sample_output, sample_hidden = encoder(example_input_batch, sample_hidden)\n",
        "print('Encoder output shape: (batch size, sequence length, units) '\n",
        "      f'{sample_output.shape}')\n",
        "print('Encoder Hidden state shape: (batch size, units) '\n",
        "      f'{sample_hidden.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Encoder output shape: (batch size, sequence length, units) (64, 53, 1024)\n",
            "Encoder Hidden state shape: (batch size, units) (64, 1024)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ppX7ygDWC8uw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BahdanauAttention(tf.keras.Model):\n",
        "    def __init__(self, units):\n",
        "        super(BahdanauAttention, self).__init__()\n",
        "        self.W1 = tf.keras.layers.Dense(units)\n",
        "        self.W2 = tf.keras.layers.Dense(units)\n",
        "        self.V = tf.keras.layers.Dense(1)\n",
        "\n",
        "    def call(self, query, values):\n",
        "        # hidden shape == (batch_size, hidden size)\n",
        "        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n",
        "        # we are doing this to perform addition to calculate the score\n",
        "        hidden_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "        # score shape == (batch_size, max_length, 1)\n",
        "        # we get 1 at the last axis because we are applying score to self.V\n",
        "        # the shape of the tensor before applying self.V is\n",
        "        # (batch_size, max_length, units)\n",
        "        score = self.V(tf.nn.tanh(\n",
        "            self.W1(values) + self.W2(hidden_with_time_axis)))\n",
        "\n",
        "        # attention_weights shape == (batch_size, max_length, 1)\n",
        "        attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "        # context_vector shape after sum == (batch_size, hidden_size)\n",
        "        context_vector = attention_weights * values\n",
        "        context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "        return context_vector, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yJ_B3mhW3jFk",
        "colab": {}
      },
      "source": [
        "class Decoder(tf.keras.Model):\n",
        "    def __init__(\n",
        "        self,\n",
        "        vocab_size: int,  # 語彙数\n",
        "        embedding_dim: int,  # 埋め込みレイヤーで出力する密ベクトルの次元数\n",
        "        dec_units: int,  # GRU の出力次元数\n",
        "        batch_sz: int):  # バッチサイズ\n",
        "\n",
        "        super(Decoder, self).__init__()\n",
        "        self.batch_sz = batch_sz\n",
        "        self.dec_units = dec_units\n",
        "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "        self.gru = tf.keras.layers.GRU(\n",
        "            self.dec_units,\n",
        "            return_sequences=True,  # 入力の全単語分の出力を返す\n",
        "            return_state=True,  # 隠れ状態を返す\n",
        "            recurrent_initializer='glorot_uniform')\n",
        "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
        "        \n",
        "        # used for attention\n",
        "        self.attention = BahdanauAttention(self.dec_units)\n",
        "        \n",
        "    def call(self, x, hidden, enc_output):\n",
        "        # enc_output shape == (batch_size, max_length, hidden_size)\n",
        "        context_vector, attention_weights = self.attention(hidden, enc_output)\n",
        "\n",
        "        # x shape after passing through embedding ==\n",
        "        # (batch_size, 1, embedding_dim)\n",
        "        x = self.embedding(x)\n",
        "        \n",
        "        # x shape after concatenation == \n",
        "        # (batch_size, 1, embedding_dim + hidden_size)\n",
        "        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis=-1)\n",
        "        \n",
        "        # passing the concatenated vector to the GRU\n",
        "        output, state = self.gru(x)\n",
        "        \n",
        "        # output shape == (batch_size * 1, hidden_size)\n",
        "        output = tf.reshape(output, (-1, output.shape[2]))\n",
        "        \n",
        "        # output shape == (batch_size, vocab)\n",
        "        x = self.fc(output)\n",
        "        \n",
        "        return x, state, attention_weights"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "P5UY8wko3jFp",
        "outputId": "5c265de9-ddc6-4b1f-f6de-7f9e84456135",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)\n",
        "\n",
        "sample_decoder_output, _, _ = decoder(\n",
        "    tf.random.uniform((BATCH_SIZE, 1)),\n",
        "    sample_hidden,\n",
        "    sample_output)\n",
        "\n",
        "print('Decoder output shape: (batch_size, vocab size) '\n",
        "      f'{sample_decoder_output.shape}')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Decoder output shape: (batch_size, vocab size) (64, 12934)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "_ch_71VbIRfK"
      },
      "source": [
        "## Define the optimizer and the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WmTHr5iV3jFr",
        "colab": {}
      },
      "source": [
        "optimizer = tf.keras.optimizers.Adam()\n",
        "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
        "    from_logits=True,\n",
        "    reduction='none')\n",
        "\n",
        "def loss_function(real, pred):\n",
        "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
        "    loss_ = loss_object(real, pred)\n",
        "    \n",
        "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
        "    loss_ *= mask\n",
        "\n",
        "    return tf.reduce_mean(loss_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CrO56RpJ3OV_",
        "colab_type": "text"
      },
      "source": [
        "> [Adam](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam)\n",
        "> <br />\n",
        "> [SparseCategoricalCrossentropy](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/losses/SparseCategoricalCrossentropy)\n",
        "> <br />\n",
        "> [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "DMVWzzsfNl4e"
      },
      "source": [
        "## Checkpoints (Object-based saving)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Zj8bXQTgNwrF",
        "colab": {}
      },
      "source": [
        "checkpoint_dir = './training_checkpoints'\n",
        "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
        "checkpoint = tf.train.Checkpoint(\n",
        "    optimizer=optimizer,\n",
        "    encoder=encoder,\n",
        "    decoder=decoder)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ubi7sTRv3Tey",
        "colab_type": "text"
      },
      "source": [
        "> [Checkpoint](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/train/Checkpoint)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "hpObfY22IddU"
      },
      "source": [
        "## Training\n",
        "\n",
        "1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n",
        "<br />出力と隠れ状態を返すエンコーダーに入力を渡します。\n",
        "2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n",
        "<br />エンコーダーの出力と隠れ状態、およびデコーダー入力（開始トークン）がデコーダーに渡されます。\n",
        "3. The decoder returns the *predictions* and the *decoder hidden state*.\n",
        "<br />デコーダーは予測結果とデコーダー隠れ状態を返します。\n",
        "4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n",
        "<br />デコーダー隠れ状態がモデルに戻され、予測結果が損失の計算に使用されます。\n",
        "5. Use *teacher forcing* to decide the next input to the decoder.\n",
        "<br />デコーダーへの次の入力を決定するために *teacher forcing* を使用します。\n",
        "6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n",
        "<br />*Teacher forcing* は目標（正解）の単語を次の入力としてデコーダーに渡す手法です。\n",
        "7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n",
        "<br />最後のステップは勾配を計算し、それをオプティマイザーに適用して逆伝播することです。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w93Jp3bxMAPF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "@tf.function\n",
        "def train_step(inp, targ, enc_hidden):\n",
        "    loss = 0\n",
        "\n",
        "    with tf.GradientTape() as tape:\n",
        "        enc_output, enc_hidden = encoder(inp, enc_hidden)\n",
        "\n",
        "        dec_hidden = enc_hidden\n",
        "\n",
        "        dec_input = tf.expand_dims(\n",
        "            [targ_lang.word_index['<start>']] * BATCH_SIZE, 1)\n",
        "\n",
        "        # Teacher forcing - feeding the target as the next input\n",
        "        for t in range(1, targ.shape[1]):\n",
        "            # passing enc_output to the decoder\n",
        "            predictions, dec_hidden, _ = decoder(\n",
        "                dec_input,\n",
        "                dec_hidden,\n",
        "                enc_output)\n",
        "\n",
        "            loss += loss_function(targ[:, t], predictions)\n",
        "\n",
        "            # using teacher forcing\n",
        "            dec_input = tf.expand_dims(targ[:, t], 1)\n",
        "\n",
        "    batch_loss = (loss / int(targ.shape[1]))\n",
        "\n",
        "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
        "\n",
        "    gradients = tape.gradient(loss, variables)\n",
        "\n",
        "    optimizer.apply_gradients(zip(gradients, variables))\n",
        "    \n",
        "    return batch_loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ddefjBMa3jF0",
        "outputId": "4c5a2383-c860-4dc9-e932-246a4e8d13d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "EPOCHS = 10\n",
        "\n",
        "for epoch in range(EPOCHS):\n",
        "    start = time.time()\n",
        "\n",
        "    enc_hidden = encoder.initialize_hidden_state()\n",
        "    total_loss = 0\n",
        "\n",
        "    for (batch, (inp, targ)) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        batch_loss = train_step(inp, targ, enc_hidden)\n",
        "        total_loss += batch_loss\n",
        "\n",
        "        if batch % 100 == 0:\n",
        "            print(f'Epoch {epoch + 1} Batch {batch} '\n",
        "                  f'Loss {batch_loss.numpy():.4f}')\n",
        "\n",
        "    # saving (checkpoint) the model every 2 epochs\n",
        "    if (epoch + 1) % 2 == 0:\n",
        "        checkpoint.save(file_prefix = checkpoint_prefix)\n",
        "    \n",
        "    print(f'Epoch {epoch + 1} Loss {total_loss / steps_per_epoch:.4f}')\n",
        "    print(f'Time taken for 1 epoch {time.time() - start} sec\\n')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1 Batch 0 Loss 1.5141\n",
            "Epoch 1 Batch 100 Loss 0.8705\n",
            "Epoch 1 Batch 200 Loss 0.8319\n",
            "Epoch 1 Batch 300 Loss 0.7555\n",
            "Epoch 1 Batch 400 Loss 0.6828\n",
            "Epoch 1 Batch 500 Loss 0.6278\n",
            "Epoch 1 Batch 600 Loss 0.7035\n",
            "Epoch 1 Batch 700 Loss 0.5597\n",
            "Epoch 1 Batch 800 Loss 0.6325\n",
            "Epoch 1 Batch 900 Loss 0.5772\n",
            "Epoch 1 Batch 1000 Loss 0.5945\n",
            "Epoch 1 Batch 1100 Loss 0.5523\n",
            "Epoch 1 Batch 1200 Loss 0.5225\n",
            "Epoch 1 Batch 1300 Loss 0.5209\n",
            "Epoch 1 Batch 1400 Loss 0.5135\n",
            "Epoch 1 Loss 0.6386\n",
            "Time taken for 1 epoch 1195.2051565647125 sec\n",
            "\n",
            "Epoch 2 Batch 0 Loss 0.4638\n",
            "Epoch 2 Batch 100 Loss 0.4154\n",
            "Epoch 2 Batch 200 Loss 0.4725\n",
            "Epoch 2 Batch 300 Loss 0.4313\n",
            "Epoch 2 Batch 400 Loss 0.3725\n",
            "Epoch 2 Batch 500 Loss 0.3484\n",
            "Epoch 2 Batch 600 Loss 0.4036\n",
            "Epoch 2 Batch 700 Loss 0.3237\n",
            "Epoch 2 Batch 800 Loss 0.3503\n",
            "Epoch 2 Batch 900 Loss 0.3102\n",
            "Epoch 2 Batch 1000 Loss 0.3135\n",
            "Epoch 2 Batch 1100 Loss 0.2782\n",
            "Epoch 2 Batch 1200 Loss 0.2917\n",
            "Epoch 2 Batch 1300 Loss 0.2927\n",
            "Epoch 2 Batch 1400 Loss 0.3001\n",
            "Epoch 2 Loss 0.3462\n",
            "Time taken for 1 epoch 1085.4666738510132 sec\n",
            "\n",
            "Epoch 3 Batch 0 Loss 0.2805\n",
            "Epoch 3 Batch 100 Loss 0.2229\n",
            "Epoch 3 Batch 200 Loss 0.2915\n",
            "Epoch 3 Batch 300 Loss 0.2738\n",
            "Epoch 3 Batch 400 Loss 0.2212\n",
            "Epoch 3 Batch 500 Loss 0.2165\n",
            "Epoch 3 Batch 600 Loss 0.2605\n",
            "Epoch 3 Batch 700 Loss 0.2022\n",
            "Epoch 3 Batch 800 Loss 0.2290\n",
            "Epoch 3 Batch 900 Loss 0.1942\n",
            "Epoch 3 Batch 1000 Loss 0.1871\n",
            "Epoch 3 Batch 1100 Loss 0.1865\n",
            "Epoch 3 Batch 1200 Loss 0.1958\n",
            "Epoch 3 Batch 1300 Loss 0.1649\n",
            "Epoch 3 Batch 1400 Loss 0.2070\n",
            "Epoch 3 Loss 0.2154\n",
            "Time taken for 1 epoch 1083.6763894557953 sec\n",
            "\n",
            "Epoch 4 Batch 0 Loss 0.1952\n",
            "Epoch 4 Batch 100 Loss 0.1489\n",
            "Epoch 4 Batch 200 Loss 0.1969\n",
            "Epoch 4 Batch 300 Loss 0.1832\n",
            "Epoch 4 Batch 400 Loss 0.1511\n",
            "Epoch 4 Batch 500 Loss 0.1447\n",
            "Epoch 4 Batch 600 Loss 0.1901\n",
            "Epoch 4 Batch 700 Loss 0.1429\n",
            "Epoch 4 Batch 800 Loss 0.1599\n",
            "Epoch 4 Batch 900 Loss 0.1424\n",
            "Epoch 4 Batch 1000 Loss 0.1212\n",
            "Epoch 4 Batch 1100 Loss 0.1396\n",
            "Epoch 4 Batch 1200 Loss 0.1468\n",
            "Epoch 4 Batch 1300 Loss 0.1082\n",
            "Epoch 4 Batch 1400 Loss 0.1502\n",
            "Epoch 4 Loss 0.1508\n",
            "Time taken for 1 epoch 1084.803064584732 sec\n",
            "\n",
            "Epoch 5 Batch 0 Loss 0.1386\n",
            "Epoch 5 Batch 100 Loss 0.1180\n",
            "Epoch 5 Batch 200 Loss 0.1519\n",
            "Epoch 5 Batch 300 Loss 0.1385\n",
            "Epoch 5 Batch 400 Loss 0.1347\n",
            "Epoch 5 Batch 500 Loss 0.1029\n",
            "Epoch 5 Batch 600 Loss 0.1395\n",
            "Epoch 5 Batch 700 Loss 0.1077\n",
            "Epoch 5 Batch 800 Loss 0.1273\n",
            "Epoch 5 Batch 900 Loss 0.1017\n",
            "Epoch 5 Batch 1000 Loss 0.0938\n",
            "Epoch 5 Batch 1100 Loss 0.1043\n",
            "Epoch 5 Batch 1200 Loss 0.1222\n",
            "Epoch 5 Batch 1300 Loss 0.0743\n",
            "Epoch 5 Batch 1400 Loss 0.1095\n",
            "Epoch 5 Loss 0.1135\n",
            "Time taken for 1 epoch 1081.821839094162 sec\n",
            "\n",
            "Epoch 6 Batch 0 Loss 0.1029\n",
            "Epoch 6 Batch 100 Loss 0.0758\n",
            "Epoch 6 Batch 200 Loss 0.1230\n",
            "Epoch 6 Batch 300 Loss 0.1055\n",
            "Epoch 6 Batch 400 Loss 0.0978\n",
            "Epoch 6 Batch 500 Loss 0.0781\n",
            "Epoch 6 Batch 600 Loss 0.1037\n",
            "Epoch 6 Batch 700 Loss 0.0815\n",
            "Epoch 6 Batch 800 Loss 0.0960\n",
            "Epoch 6 Batch 900 Loss 0.0806\n",
            "Epoch 6 Batch 1000 Loss 0.0724\n",
            "Epoch 6 Batch 1100 Loss 0.0808\n",
            "Epoch 6 Batch 1200 Loss 0.0914\n",
            "Epoch 6 Batch 1300 Loss 0.0578\n",
            "Epoch 6 Batch 1400 Loss 0.0873\n",
            "Epoch 6 Loss 0.0882\n",
            "Time taken for 1 epoch 1083.3765573501587 sec\n",
            "\n",
            "Epoch 7 Batch 0 Loss 0.0831\n",
            "Epoch 7 Batch 100 Loss 0.0646\n",
            "Epoch 7 Batch 200 Loss 0.0927\n",
            "Epoch 7 Batch 300 Loss 0.0871\n",
            "Epoch 7 Batch 400 Loss 0.0821\n",
            "Epoch 7 Batch 500 Loss 0.0627\n",
            "Epoch 7 Batch 600 Loss 0.0865\n",
            "Epoch 7 Batch 700 Loss 0.0763\n",
            "Epoch 7 Batch 800 Loss 0.0744\n",
            "Epoch 7 Batch 900 Loss 0.0753\n",
            "Epoch 7 Batch 1000 Loss 0.0779\n",
            "Epoch 7 Batch 1100 Loss 0.0664\n",
            "Epoch 7 Batch 1200 Loss 0.0840\n",
            "Epoch 7 Batch 1300 Loss 0.0553\n",
            "Epoch 7 Batch 1400 Loss 0.0793\n",
            "Epoch 7 Loss 0.0727\n",
            "Time taken for 1 epoch 1081.0256087779999 sec\n",
            "\n",
            "Epoch 8 Batch 0 Loss 0.0710\n",
            "Epoch 8 Batch 100 Loss 0.0509\n",
            "Epoch 8 Batch 200 Loss 0.0759\n",
            "Epoch 8 Batch 300 Loss 0.0681\n",
            "Epoch 8 Batch 400 Loss 0.0709\n",
            "Epoch 8 Batch 500 Loss 0.0545\n",
            "Epoch 8 Batch 600 Loss 0.0646\n",
            "Epoch 8 Batch 700 Loss 0.0545\n",
            "Epoch 8 Batch 800 Loss 0.0557\n",
            "Epoch 8 Batch 900 Loss 0.0578\n",
            "Epoch 8 Batch 1000 Loss 0.0426\n",
            "Epoch 8 Batch 1100 Loss 0.0465\n",
            "Epoch 8 Batch 1200 Loss 0.0671\n",
            "Epoch 8 Batch 1300 Loss 0.0431\n",
            "Epoch 8 Batch 1400 Loss 0.0595\n",
            "Epoch 8 Loss 0.0573\n",
            "Time taken for 1 epoch 1083.2719917297363 sec\n",
            "\n",
            "Epoch 9 Batch 0 Loss 0.0524\n",
            "Epoch 9 Batch 100 Loss 0.0428\n",
            "Epoch 9 Batch 200 Loss 0.0596\n",
            "Epoch 9 Batch 300 Loss 0.0477\n",
            "Epoch 9 Batch 400 Loss 0.0516\n",
            "Epoch 9 Batch 500 Loss 0.0462\n",
            "Epoch 9 Batch 600 Loss 0.0707\n",
            "Epoch 9 Batch 700 Loss 0.0454\n",
            "Epoch 9 Batch 800 Loss 0.0465\n",
            "Epoch 9 Batch 900 Loss 0.0468\n",
            "Epoch 9 Batch 1000 Loss 0.0307\n",
            "Epoch 9 Batch 1100 Loss 0.0479\n",
            "Epoch 9 Batch 1200 Loss 0.0543\n",
            "Epoch 9 Batch 1300 Loss 0.0308\n",
            "Epoch 9 Batch 1400 Loss 0.0563\n",
            "Epoch 9 Loss 0.0474\n",
            "Time taken for 1 epoch 1080.529726266861 sec\n",
            "\n",
            "Epoch 10 Batch 0 Loss 0.0500\n",
            "Epoch 10 Batch 100 Loss 0.0473\n",
            "Epoch 10 Batch 200 Loss 0.0557\n",
            "Epoch 10 Batch 300 Loss 0.0420\n",
            "Epoch 10 Batch 400 Loss 0.0498\n",
            "Epoch 10 Batch 500 Loss 0.0403\n",
            "Epoch 10 Batch 600 Loss 0.0503\n",
            "Epoch 10 Batch 700 Loss 0.0393\n",
            "Epoch 10 Batch 800 Loss 0.0439\n",
            "Epoch 10 Batch 900 Loss 0.0380\n",
            "Epoch 10 Batch 1000 Loss 0.0263\n",
            "Epoch 10 Batch 1100 Loss 0.0442\n",
            "Epoch 10 Batch 1200 Loss 0.0512\n",
            "Epoch 10 Batch 1300 Loss 0.0357\n",
            "Epoch 10 Batch 1400 Loss 0.0549\n",
            "Epoch 10 Loss 0.0419\n",
            "Time taken for 1 epoch 1084.0447499752045 sec\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mU3Ce8M6I3rz"
      },
      "source": [
        "## Translate\n",
        "\n",
        "* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n",
        "<br />評価関数は *teacher forcing* を使用しないことを除いてトレーニングループと似ています。各時間ステップのデコーダーへの入力は、1つ前の予測値および隠れ状態とエンコーダー出力です。\n",
        "* Stop predicting when the model predicts the *end token*.\n",
        "<br />モデルが終了トークンを予測したら予測を停止します。\n",
        "* And store the *attention weights for every time step*.\n",
        "<br />そしてタイムステップごとに attention weights を保存します。\n",
        "\n",
        "Note: The encoder output is calculated only once for one input.\n",
        "\n",
        "> 注：エンコーダー出力は1つの入力に対して1回だけ計算されます。"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "EbQpyYs13jF_",
        "colab": {}
      },
      "source": [
        "def evaluate(sentence):\n",
        "\n",
        "    attention_plot = np.zeros((max_length_targ, max_length_inp))\n",
        "    \n",
        "    sentence = preprocess_sentence(sentence)\n",
        "\n",
        "    inputs = [inp_lang.word_index[i] for i in sentence.split(' ')]\n",
        "    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n",
        "        [inputs],\n",
        "        maxlen=max_length_inp,\n",
        "        padding='post')\n",
        "    inputs = tf.convert_to_tensor(inputs)\n",
        "    \n",
        "    result = ''\n",
        "\n",
        "    hidden = [tf.zeros((1, units))]\n",
        "    enc_out, enc_hidden = encoder(inputs, hidden)\n",
        "\n",
        "    dec_hidden = enc_hidden\n",
        "    dec_input = tf.expand_dims([targ_lang.word_index['<start>']], 0)\n",
        "\n",
        "    for t in range(max_length_targ):\n",
        "        predictions, dec_hidden, attention_weights = decoder(\n",
        "            dec_input,\n",
        "            dec_hidden,\n",
        "            enc_out)\n",
        "        \n",
        "        # storing the attention weights to plot later on\n",
        "        attention_weights = tf.reshape(attention_weights, (-1,))\n",
        "        attention_plot[t] = attention_weights.numpy()\n",
        "\n",
        "        predicted_id = tf.argmax(predictions[0]).numpy()\n",
        "\n",
        "        result += targ_lang.index_word[predicted_id] + ' '\n",
        "\n",
        "        if targ_lang.index_word[predicted_id] == '<end>':\n",
        "            return result, sentence, attention_plot\n",
        "        \n",
        "        # the predicted ID is fed back into the model\n",
        "        dec_input = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "    return result, sentence, attention_plot"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "s5hQWlbN3jGF",
        "colab": {}
      },
      "source": [
        "# function for plotting the attention weights\n",
        "def plot_attention(attention, sentence, predicted_sentence):\n",
        "    fig = plt.figure(figsize=(5, 5))\n",
        "    ax = fig.add_subplot(1, 1, 1)\n",
        "    ax.matshow(attention, cmap='viridis')\n",
        "    \n",
        "    fontdict = {'fontsize': 11}\n",
        "    \n",
        "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
        "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
        "\n",
        "    plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sl9zUHzg3jGI",
        "colab": {}
      },
      "source": [
        "def translate(sentence):\n",
        "\n",
        "    result, sentence, attention_plot = evaluate(sentence)\n",
        "        \n",
        "    print(f'Input: {sentence}')\n",
        "    print(f'Predicted translation: {result}')\n",
        "    \n",
        "    attention_plot = attention_plot[\n",
        "        :len(result.split(' ')), :len(sentence.split(' '))]\n",
        "    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "n250XbnjOaqP"
      },
      "source": [
        "## Restore the latest checkpoint and test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UJpT9D5_OgP6",
        "outputId": "e14c5728-4182-45ff-d4c0-3d8365ad4322",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# restoring the latest checkpoint in checkpoint_dir\n",
        "checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f8e43604588>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WrAM0FDomq3E",
        "outputId": "3bb2349f-9b44-40af-f33c-075b09d520ce",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "translate('hace mucho frio aqui.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> hace mucho frio aqui . <end>\n",
            "Predicted translation: it s very cold here . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAFUCAYAAABxzmrnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nEVX5/0EgxHesm2MAAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zSx2iM36EZQZ",
        "outputId": "40389d19-a9e1-41c9-9e49-370a91907e09",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "translate('esta es mi vida.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> esta es mi vida . <end>\n",
            "Predicted translation: this is my life . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAFUCAYAAACk3L6qAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "A3LLCx3ZE0Ls",
        "outputId": "25e8664e-3a95-4292-dacb-59e4cfc09478",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 391
        }
      },
      "source": [
        "translate('todavia estan en casa?')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> todavia estan en casa ? <end>\n",
            "Predicted translation: they re still at home . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAS0AAAFUCAYAAABxzmrnAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAGERJREFUeJzt3XuUZXVh5fHv7m6a7ub90Bg0CAF5\nQ6M04KgQTFyDUYKzBBVEEmKcTsCoeWg0PoImzpgoPoYEJqs1BERAozIERqAHJCAqCE1DI0EJQkBB\n5SXy7veeP84pvFVWdxdU3fM7v679WatWnXvurXt2Vffd9zx/V7aJiKjFjNIBIiKeiZRWRFQlpRUR\nVUlpRURVUloRUZWUVkRUJaUVEVVJaUVEVVJaEVGVlFZEVCWlNYakTSU9LOnQ0lki4pfNKh2gh14L\n3Ae8BfhGqRCStgJ2B+aMzLNdLE9EX6S0ftmbgZOARZI2sb2q6wCS3gScAmwD3AvsCiwDXtJ1loi+\nyebhgHbtZl/bVwBX0Kx1lfB+4ADgdtu7A68Gri+UJaJXUlqjHQX8azt9Hs1aVwmrbd9PuyZs+zLg\nwEJZInolpTXam4Fz2+krgQWStiiQY4UkAbdLeoek3wE2L5AjYr1KHLhKabUkbQvMtH0TgJvREb8A\nvLJAnA8CWwLvBV4H/BXNfraIvhk8cNUJZeTSiHi2JH0FOB1YBOzZxYGrHD0cIOkztv9kQ/OGuPw3\n2P6ypHHXqmyf3kWOiIkYPHAlaeTA1QXDXm5Ka7Txtst/o8Pl7wN8mfF3umeVOPpm7IGrE+mgtLJ5\nSLOGA7wR+C3g8oG7tgI2t/3yIsEiekzS5cC7bd/UHji6A5hv+7FhLjdrWo3/AL4GHNR+H/Eo8PWu\nw0i6AzgDOMv2PV0vP2JDxjtwJWnkwNWFQ1121rQakmYCn7L9rh5k2Qc4ATgWuBX4Z+B828s7zrE7\nzZHMXRh4g7N9UJc5IgaltAZIuq5PL8i2SH8beBtwqO1tO17+jTT72K4F1ozMt31VlzmiXyTNW9/9\ntp8c5vKzeTja1yS9G/g88PjIzGH/I6zHHsBhNDvmbyiw/Bm2/2eB5Ua/Pc76DwzNHObCU1qjndx+\n/zjNP4ra70P9RxhL0juB36M5C/4s4KW2f9RlhtY1kvazfXOBZUdP2Z4BIOmDwAqac7REs0Uwe9jL\nz+ZhD0n6LHCm7W8VznEjsBdwG/D0/rQ+bUJHOZKW2n7JmHk32D5gmMvNmlYP2f7vpTO0Ojmptibt\nwYn5jB7n7PPlEhU1V9Kutn8AIGkXYL37u6ZCSmuApPnAP9L8p9x0ZL7trjcPf41mE3Xsi+PXu8yR\nHe6jtZvtfwj8Ks1QQYcAV9HsA52OPgBcK+kGms3D/YGFw15oSmu002kO8X+KZgyrtwNDPVFuHc4A\nvkhTWsfRnGl8R9ch2ss03kvzn3GwPH+z6yw9sZDmXL5v2T68PTXlrwpnKsb2+ZK+RfM3AbjW9gPD\nXm5GeRhtju2v0xw1+4ntDwJHF8ixve1/AtbYvobmnK3XFMhxBs2pDrsBn22nryuQoy+W234CmCFJ\ntm+h+dtMW7bvAxbTnIT9xIZOh5gKKa3RVrfffyZpvqTtgO0L5FjZfn9c0o7AJsBzCuTY1faHgCdt\nnwccwfjXZ04XT0rahGbo67+T9A46PrLcJ5JeL+ke4CmaLZLH6WDLJKU12pfaovoY8E3gRzSbjF37\nRnuZxOk052fdwZAvjViHFe33lW2elZQpz744ieaQ/p8D29JcTH980URlfZzmmt1NbM+0PaOL/b85\n5WEd2nfUOcO++HMCOXYEtmw3Rbpe9heAd9Jsnv4R8HPgTtvHdJ0l+qfUFSQprQGSvmn7FRuaN8Tl\n77W++23f2kWO8Uh6BbA1cIntNRt6/MZI0ieBvwaeAP6N5tOR/tD2F4oGK0TSX9K8kX2J0efxDfUK\nkpTWgLEny7XX/n3X9nrLZAqX/5/84kz8HWlGmTDNEDk/tL1zFzkG8hQdFLFvJC2zPV/Sa2k2C/8c\n+Jrt/QtHK0LS2oGbT19BMuxNxOzTAiS9R9IDwD6S7h/5Ah4Bru4qh+2d23Ox/i9wjO1t2ouk3wRc\n1FWOAaUHReyrQ2lG3biXaTw4Y7sPa+Qr+7S61J6PtA3wDzTnZo141PbDBfIssz1/zLybunpHz6CI\n45N0GfADmpE3XkxzpGyp7f2KBitI0m40Y8P/q6TNgdm2fzbMZWZNC7D9iO27bB9h+27bd9Mcxt29\nUCRJOmTgxsvp9t9qZFDEx9rvI1//SPOCna7eTHMd5jHtm9kLaE5EnpYknUBzVPvT7aznA/8y9OVm\nTesXJF1Ncy6SgH+n2cl4se33dJzjEJoxt59os8wBju36AmpJ2w77XXOCOV5Gc3j912mu4hjZd/Lc\nosGmufaC+kOAq22/uJ13i+19hrncrGmNtrntR2iK6xxgX5rLeTpl+2qaF+jRNB8esGuhER8+IGkr\nSbMkXS3pCUmdfb7dgH+iOWftFTRjiy2gwCduS/o1SedJulXSnSNfXefokZW2Hx8zb/W4j5xCKa3R\nRi6SfiVwme21dPCPsA670uzwPZSmwEp4VVvihwP30lyy8u4COZ6yfa7tO0c239tN+K6dQbOPTzTX\nhH6TZryz6eqhdp+WAdo3tKF/pkFKa7QrJd1Ks8p7paStGRhmuCuSjgcuo7lQeX/gcknHdZ1jQOmj\nZRdL6sO+tJFrQlcXvia0L/4UOBfYXdJdwPuBoX/GQvZpjSFpf5qzvh9tL+nZ0faNHWdYBhxu+6ft\n7ecBi8ceUewgRy+OlrWno2zXLn8FhfZpSfqO7YMlXUNzGsp9wPe6HjKoT9pzGXej+Te5rYsTjzM0\nTau9tu47tl80MPtjNGs8nZYWwEhhjUw3HyvXuTe3X1+w/bCknfjFkaIuLaA5G38W8CDNC6TEH2Ts\nNaEraD74Y9oZ83r5XjtvkaTLbA/1b5LNw1Z7lOx77ekFI9ce/lfKXKh8h6SPSNqh/ToZKLHDdwHN\nPqxz2tvbA68vkGMvmhNuv9ruy9oeOLVAjsU0wwWdDRwAHEOZk36LK/l6SWmNdg7NZw1Cs/P5Stsr\n1vP4YfkjmnPEbqYZBmUPOhgRchx/TXOU7mEA20soc1BgvBy7FMjxCZpLq7D9Q+DbwCldLVzSGyQ9\nLOk+SW/sarnrUeT1ks3D0S4E/oekGTTvoqWODF06zgcGLKW5QLdT42yarlzXYzvOUeLNRB7YCWx7\nbbtPpyt/A7wcmEszjNJuwAXA3TQHCf6zwyxQ6PWSNa0Btp+iefd8HXAwoy9hGbr2fKh5NCNjzpU0\nr/3+q3TwgQHjeEzSr/CLQ9qH0ZxwO51zHDxyo51+osPlr7B9q+0baI5w7wecT3M96N93mAMo+Hqx\nna+BL5rV3HuATxdY9snAWprTLNYOfP0c+FCBPAcBS2k2y66kOVfrgGmc478AP6F5cV4O/JjmMym7\nWv7FJX7vDWTq/PWSUx7GaFf3bwDeantpoQz/YPuPSyx7rPZi8pfRHK37tu0Sazh9yrENTXkBXOMO\nL6iX9Bxglu2fdLXMDSnxeklpRURVsk8rIqqS0oqIqqS01kFSifOifklyjJYco03HHCmtdevFfwaS\nY6zkGG3a5UhpRURVNsqjh7M1x3O12aSeYyUrmP308FrPztot507q5wFWrXyCTWZP7nfZecf7Jp3j\noZ+tZbttJ/ced/f3t5l0jpVrn2L2jEn+XTX59+qVa55k9sxJnu87BZd8r1zzFLNnTvbvMfkgK1c/\nyexZk/t7PLr8pw/a3uCHAW+Ul/HM1Wa8dE75YY6ePLQfn3dw9un9GMb8xIOOKh2hMWdyb0ZTxZv0\n5OU3ox8bXIu//7cTGtixH2kjIiYopRURVUlpRURVUloRUZWUVkRUJaUVEVVJaUVEVVJaEVGVlFZE\nVCWlFRFVSWlFRFWGWlqSPixpdjt9pqRejHseEfUa9prWycDsIS8jIqaRoZWWpNPayW9LugnYGthH\n0hWSbpf0ebWfvilpS0mfk3SdpJsl/S9JMyUdKOmWMc+7TNLLhpU7IvptaKVl++3t5Mts70/z2X37\nAK8B9gYOAF7VPuZTwFW2DwL2B55L85FE1wOPS/oNAEmHAGttf3vs8iQtlLRE0pKVRT58OCK60PWO\n+AtsL7e9kubDN3dp5x8JvKddI1tKU2i7tfedCpzUTr8dOI1x2F5ke4HtBZMdvC8i+qvrUciWD0yv\nGVi+gP9m+85xfubLwMckvRh4JfDW4UaMiD4b9prWY8BWE3jchcD72k+rRdL2knYGsL0KOKN9zDm2\nnxxW2Ijov2GX1ieBKwZ2xK/Ln9CseS2T9F3gUuD5A/d/rr39v4cVNCLqMNTNQ9sfAT6yjvtOGJh+\nDDhxPU/1SuAS27dPacCIqE5PRtZfN0mLaXbYH1k6S0SU1/vSsn146QwR0R+59jAiqpLSioiqpLQi\noioprYioSkorIqqS0oqIqqS0IqIqKa2IqEpKKyKq0vsz4p8N26xdUX4gwHuPXVU6AgA7ztq8dAQA\n1v78kdIRAJix3balIwDgn95fOgJAL14rz0TWtCKiKimtiKhKSisiqpLSioiqpLQioioprYioSkor\nIqqS0oqIqqS0IqIqKa2IqEpKKyKqktKKiKqktCKiKimtiKhKr0tL0kY5dE5EPHu9Ky1JlvRhSdcD\nJ7fz3ivpOklLJV0k6XmFY0ZEIX1dk3nK9oEAkt4C7AK81PZaSScCnwSOG/wBSQuBhQBzmNdx3Ijo\nSl9L66yB6SOBBcBSSdBk/qUhMG0vAhYBbKlt3UHGiCigr6X1+MC0gI/aPqNUmIjoj97t0xrHhcBJ\nkrYBkLSppPmFM0VEIX1d03qa7bMlbQ9c1W4ezgBOB5YVDRYRRfSutGxrnHmfBj5dIE5E9EwNm4cR\nEU9LaUVEVVJaEVGVlFZEVCWlFRFVSWlFRFVSWhFRlZRWRFQlpRURVUlpRURVUloRUZXeXXs4FTR7\nE2Y97/mlY7Dm0U1KRwDgwKVvLB0BgG1eMrd0BABWzu3Hf/tNb1ldOkLjgYdKJ3hGsqYVEVVJaUVE\nVVJaEVGVlFZEVCWlFRFVSWlFRFVSWhFRlZRWRFQlpRURVUlpRURVUloRUZWUVkRUJaUVEVUpXlqS\ndpK0cMy8iyXt0k5fKemIdvpMSX9cImdE9EPx0gJ2AkaVlu3X2L6jTJyI6LNOBxaSNA84C9gbWAXc\n1k7vLOkm4Ae2j5Z0F3CE7Vu6zBcR/df1aGiHA1va3gtA0jbAfOAU2wsm88TtJuZCgDkzt5hszojo\nqa43D5cBe0o6TdIbgBVT9cS2F9leYHvB7Jn9GCEzIqZep6Vl+06azcHLgFfRlNicLjNERN263qf1\nAuBnti+Q9P+AHwOPAlt1mSMi6tX15uG+wDWSlgHXAR9rv98m6RZJX+k4T0RUptM1LduXAJeMc9cR\nYx6308D0YQPTJwwpWkRUog/naUVETFhKKyKqktKKiKqktCKiKimtiKhKSisiqpLSioiqpLQioiop\nrYioSkorIqqS0oqIqnQ9CGAnvHIVq390T+kY7P7O+0tHAMCrV5WOAMDtpx5UOgIAnzj8vNIRAPjM\n+44tHQGAeRc8VDrCM5I1rYioSkorIqqS0oqIqqS0IqIqKa2IqEpKKyKqktKKiKqktCKiKimtiKhK\nSisiqpLSioiqpLQioioprYioSjWlJWknSQtL54iIsqopLWAnIKUVMc31srQknSNpiaTvSvo/krYB\nTgP2knSTpK+UzhgRZfR1EMB32X4QQNJHgfcCbwdOsb1gvB9oNx0XAsxhXlc5I6JjfS2t35V0HDAb\n2Az4D+DS9f2A7UXAIoAtta2HnjAiiujd5qGkQ4ATgVfb3hf4IDCnbKqI6IvelRawNfAI8JCkTYG3\ntvMfBbYqlioieqGPpXUpcAfNJuFVwNJ2/s3AbZJuyY74iOmrd/u0bK8C3rSOu4/oMktE9E8f17Qi\nItYppRURVUlpRURVUloRUZWUVkRUJaUVEVVJaUVEVVJaEVGVlFZEVCWlFRFVSWlFRFV6d+3hlJg3\nB+2xd+kU+MZ/Lx2hV/b81E9LRwDgxkNeWDoCAPe8dk3pCADsdcMOpSM07prYw7KmFRFVSWlFRFVS\nWhFRlZRWRFQlpRURVUlpRURVUloRUZWUVkRUJaUVEVVJaUVEVVJaEVGVlFZEVCWlFRFVmXRpSbKk\nzaciTETEhmRNKyKqMlWl9U5J10u6U9JRIzMlvVrSjZJulvR1Sbu28w+TtEzSZyV9V9JSSXtL+hdJ\nt0paLGmz9rGzJX1C0nXtz5ydNbuI6WuqSutR2wcCxwOnAkh6LnA2cJzt/YBzgXMGfmYv4DTb+wLX\nAIuBP7O9F7AGOLZ93F8Aj9g+yPZ84MfAX44NIGmhpCWSlqxa/eQU/VoR0TdTNXLpF9vv1wI7SJoD\nHAwss31re98/A6dL2qK9fZvtm9rppcALbd/T3r4B2LWdPhLYUtLR7e1NgWVjA9heBCwC2HKzHTw1\nv1ZE9M1UldZyANtrJE30eZcPTK8Z5/bcdlrASbavmIKcEVG5Ye6IvxaYL2mP9vbvATfafuwZPs+F\nwJ9JmgsgaQtJe05hzoioyNA+2ML2A5KOB86VNAt4AHjLs3iqvwU+DFwvaS1g4CPA96Yqa0TUY9Kl\nZVvrum37UuDScX7mSmDBwO0zgTMHbn94YHoV8IH2KyKmuZynFRFVSWlFRFVSWhFRlZRWRFQlpRUR\nVUlpRURVUloRUZWUVkRUJaUVEVVJaUVEVVJaEVGVoV0wXZJWrWHm/Q+XjsFqacMP6oL7MbyYH+/H\n4IwXnXVI6QgA/O4JV5WOAMB3tt+/dITGXRN7WNa0IqIqKa2IqEpKKyKqktKKiKqktCKiKimtiKhK\nSisiqpLSioiqpLQioioprYioSkorIqqS0oqIqqS0IqIqKa2IqEpKKyKqktKKiKpsNIMASloILASY\nM3OLwmkiYlg2mjUt24tsL7C9YPaMuaXjRMSQbDSlFRHTQ0orIqpSXWlJuljSgtI5IqKM6nbE235N\n6QwRUU51a1oRMb2ltCKiKimtiKhKSisiqpLSioiqpLQioioprYioSkorIqqS0oqIqqS0IqIqKa2I\nqEp11x5OxOYvWsHLv3RH6Rh84yX9GIzQq1eXjgCAH3usdAQAXvCVH5aOAMDl9x5SOgIAK/dT6QiN\nJRN7WNa0IqIqKa2IqEpKKyKqktKKiKqktCKiKimtiKhKSisiqpLSioiqpLQioioprYioSkorIqqS\n0oqIqqS0IqIqvSktSZa0eekcEdFvz7q0JG03lUG6eu6IqNszKi1J8yQdI+ki4Lp23u6SLpF0vaRl\nkn5/4PGW9P72vjslHTVw3+slfV/STZI+NGZRX5V0haQ/kLT1ZH7BiNi4bLC0JM2S9GpJZwO3AIcC\nHwd2lTQLOBf4U9sHAq8A3idpj4GneLS973jg1PY5fwX4LPA62/sDKwaXafsw4N3AHsD1ks6XdJSk\nOevJuVDSEklLnnh45UR//4iozERGLr0BeA7wDuD3bT89DKak3YA9gS9KT49+uGk77/vt7S+2368F\ndmiL52Bgqe3b2vsWAX83uFDbS4Glkv4C+E3gdOAMYKvxQtpe1D4Pz997a0/g94qICk2ktP6AZi3p\n48BRks4DFtteCQh4sF1bWpflALbXtMU2oSGeJc0ADgOOBX4LuAI4ZyI/GxEbrw1uHtpeYvtdwG7A\nWcDRwO2SPgfcBjwp6fiRx0vaQ9KWG3jaa4EXS3pRe/ttg3dK+hvgDuAk4BJgT9tvs/1vE/y9ImIj\nNeEPtrC9BlgMLJY0FzjS9mpJvwN8RtJ7gJnAfcAbN/Bc90taCFwk6Sngq2MecgNwiu1HnsHvEhHT\nwLP6NB7bTwFfaqdvB167jsdpXbdtnw+cP3D3Rwfuu+DZ5IqIjV9vTi6NiJiIlFZEVCWlFRFVSWlF\nRFVSWhFRlZRWRFQlpRURVUlpRURVUloRUZWUVkRUJaUVEVV5Vtce9t1jt87gqv3mlo4BrN7wQ6aR\ntcuXl44AwNof3VM6AgCb9yRHbbKmFRFVSWlFRFVSWhFRlZRWRFQlpRURVUlpRURVUloRUZWUVkRU\nJaUVEVVJaUVEVVJaEVGVlFZEVCWlFRFVSWlFRFVSWhFRlZRWRFRloxkEUNJCYCHAHOYVThMRw7LR\nrGnZXmR7ge0Fm7Bp6TgRMSQbTWlFxPSQ0oqIqqS0IqIqKa2IqEpKKyKqktKKiKqktCKiKimtiKhK\nSisiqpLSioiqpLQioioprYioSkorIqqS0oqIqqS0IqIqsl06w5ST9ABw9ySfZnvgwSmIM1nJMVpy\njLYx5Xih7eds6EEbZWlNBUlLbC9IjuRIjn7lyOZhRFQlpRURVUlprdui0gFayTFacow27XJkn1ZE\nVCVrWhFRlZRWRFQlpRURVUlpRURVUloRUZX/DxifFOkDMRjlAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DUQVLVqUE1YW",
        "outputId": "017b3ed6-443c-40b3-f5fd-d5ad2562d22d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 410
        }
      },
      "source": [
        "# wrong translation\n",
        "translate('trata de averiguarlo.')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input: <start> trata de averiguarlo . <end>\n",
            "Predicted translation: try to figure it . <end> \n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAASgAAAFnCAYAAAAVCdSgAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n/wNdUXygnfigSwAAAABJRU5ErkJggg==\n",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "RTe5P5ioMJwN"
      },
      "source": [
        "## Next steps\n",
        "\n",
        "* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n",
        "<br />[別のデータセットをダウンロード](http://www.manythings.org/anki/)して、たとえば英語からドイツ語、または英語からフランス語への翻訳を試してください。\n",
        "* Experiment with training on a larger dataset, or using more epochs\n",
        "<br />より大きなデータセットでのトレーニング、またはより多くのエポックの使用を試してください。"
      ]
    }
  ]
}