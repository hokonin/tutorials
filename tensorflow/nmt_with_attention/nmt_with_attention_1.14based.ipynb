{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"nmt_with_attention_1.9base.ipynb","version":"0.3.2","provenance":[{"file_id":"https://github.com/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb","timestamp":1561962967730}],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"colab_type":"text","id":"AOpGoE2T-YXS"},"source":["##### Copyright 2018 The TensorFlow Authors.\n","\n","Licensed under the Apache License, Version 2.0 (the \"License\").\n","\n","# Neural Machine Translation with Attention\n","\n","<table class=\"tfo-notebook-buttons\" align=\"left\"><td>\n","<a target=\"_blank\"  href=\"https://colab.research.google.com/github/tensorflow/tensorflow/blob/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\">\n","    <img src=\"https://www.tensorflow.org/images/colab_logo_32px.png\" />Run in Google Colab</a>  \n","</td><td>\n","<a target=\"_blank\"  href=\"https://github.com/tensorflow/tensorflow/tree/master/tensorflow/contrib/eager/python/examples/nmt_with_attention/nmt_with_attention.ipynb\"><img width=32px src=\"https://www.tensorflow.org/images/GitHub-Mark-32px.png\" />View source on GitHub</a></td></table>"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"CiwtNgENbx2g"},"source":["This notebook trains a sequence to sequence (seq2seq) model for Spanish to English translation using [tf.keras](https://www.tensorflow.org/programmers_guide/keras) and [eager execution](https://www.tensorflow.org/programmers_guide/eager). This is an advanced example that assumes some knowledge of sequence to sequence models.\n","\n","> このノートブックは [tf.keras](https://www.tensorflow.org/programmers_guide/keras) と [eager exicution](https://www.tensorflow.org/programmers_guide/eager) を使用してスペイン語を英語に翻訳するシーケンス to シーケンス（seq2seq）モデルを学習します。これはシーケンス to シーケンスモデルに関する知識があることを前提とした高度な例です。\n","\n","After training the model in this notebook, you will be able to input a Spanish sentence, such as *\"¿todavia estan en casa?\"*, and return the English translation: *\"are you still at home?\"*\n","\n","> このノートブックでモデルを訓練した後は *\"¿todavia estan en casa?\"* などのスペイン語の文章を入力して、英語の翻訳 *\"are you still at home?\"* を返すことができます。\n","\n","The translation quality is reasonable for a toy example, but the generated attention plot is perhaps more interesting. This shows which parts of the input sentence has the model's attention while translating:\n","\n","> 翻訳の質は例題としては妥当ですが、生成された Attention プロットはおそらくもっと興味深いものです。これは翻訳中に入力文のどの部分にモデルが注目したかを示します。\n","\n","<img src=\"https://tensorflow.org/images/spanish-english.png\" alt=\"spanish-english attention plot\">\n","\n","Note: This example takes approximately 10 mintues to run on a single P100 GPU.\n","\n","> 注：この例では1台のP100 GPUで実行するのに約10分かかります。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"tnxXKDjq3jEL","outputId":"aa8d97dd-75ab-49ac-df28-6e46c76098c3","executionInfo":{"status":"ok","timestamp":1562222789096,"user_tz":-540,"elapsed":5726,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":394}},"source":["from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","from __future__ import unicode_literals\n","\n","# Import TensorFlow >= 1.10 and enable eager execution\n","!pip install tensorflow-gpu==2.0.0-beta1\n","import tensorflow as tf\n","# tf.enable_eager_execution()\n","\n","import matplotlib.pyplot as plt\n","from sklearn.model_selection import train_test_split\n","\n","import unicodedata\n","import re\n","import numpy as np\n","import os\n","import io\n","import time\n","\n","print(tf.__version__)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: tensorflow-gpu==2.0.0-beta1 in /usr/local/lib/python3.6/dist-packages (2.0.0b1)\n","Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.2.2)\n","Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (3.7.1)\n","Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.12.0)\n","Requirement already satisfied: tb-nightly<1.14.0a20190604,>=1.14.0a20190603 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0a20190603)\n","Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019060502,>=1.14.0.dev2019060501 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.14.0.dev2019060501)\n","Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.11.2)\n","Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.0.8)\n","Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.15.0)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.33.4)\n","Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.1.7)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.7.1)\n","Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.1.0)\n","Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (0.8.0)\n","Requirement already satisfied: numpy<2.0,>=1.14.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu==2.0.0-beta1) (1.16.4)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-beta1) (41.0.1)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (0.15.4)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tb-nightly<1.14.0a20190604,>=1.14.0a20190603->tensorflow-gpu==2.0.0-beta1) (3.1.1)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow-gpu==2.0.0-beta1) (2.8.0)\n","2.0.0-beta1\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"RmnQwTz731se","colab_type":"code","colab":{}},"source":["from typing import List, Dict, Sequence, Tuple, Iterable"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"wfodePkj3jEa"},"source":["## Download and prepare the dataset\n","\n","We'll use a language dataset provided by http://www.manythings.org/anki/. This dataset contains language translation pairs in the format:\n","\n","> http://www.manythings.org/anki/ で提供されている言語データセットを使用します。このデータセットには次の形式の言語翻訳ペアが含まれています。\n","\n","```\n","May I borrow this book?\t¿Puedo tomar prestado este libro?\n","```\n","\n","There are a variety of languages available, but we'll use the English-Spanish dataset. For convenience, we've hosted a copy of this dataset on Google Cloud, but you can also download your own copy. After downloading the dataset, here are the steps we'll take to prepare the data:\n","\n","> さまざまな言語がありますが、ここでは英語 - スペイン語のデータセットを使用します。便宜上、このデータセットのコピーをGoogle Cloudでホストしていますが、自分のコピーをダウンロードすることもできます。データセットをダウンロードした後、データを準備するための手順は次のとおりです。\n","\n","1. Add a *start* and *end* token to each sentence.<br />各文に開始トークンと終了トークンを追加します。\n","2. Clean the sentences by removing special characters.<br />特殊文字を削除して文章をきれいにします。\n","3. Create a word index and reverse word index (dictionaries mapping from word → id and id → word).<br />単語インデクスと逆単語インデクスを作成します（単語→id と id→単語 のマッピング辞書）。\n","4. Pad each sentence to a maximum length.<br />各文を最大長まで埋めます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"kRVATYOgJs1b","colab":{}},"source":["# Download the file\n","path_to_zip = tf.keras.utils.get_file(\n","    'spa-eng.zip',\n","    origin = 'http://storage.googleapis.com/'\n","        'download.tensorflow.org/data/spa-eng.zip', \n","    extract = True)\n","\n","path_to_file = os.path.dirname(path_to_zip) + \"/spa-eng/spa.txt\""],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"rd0jw-eC3jEh","colab":{}},"source":["# Converts the unicode file to ascii\n","def unicode_to_ascii(s : str) -> str :\n","    return ''.join(\n","        c for c in unicodedata.normalize('NFD', s)\n","        if unicodedata.category(c) != 'Mn')\n","\n","def preprocess_sentence(w : str) -> str :\n","    w = unicode_to_ascii(w.lower().strip())\n","    \n","    # creating a space between a word and the punctuation following it\n","    # eg: \"he is a boy.\" => \"he is a boy .\" \n","    # Reference:- https://stackoverflow.com/questions/3645931\n","    #     /python-padding-punctuation-with-white-spaces-keeping-punctuation\n","    w = re.sub(r\"([?.!,¿])\", r\" \\1 \", w)\n","    w = re.sub(r'[\" \"]+', \" \", w)\n","    \n","    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n","    w = re.sub(r\"[^a-zA-Z?.!,¿]+\", \" \", w)\n","    \n","    w = w.rstrip().strip()\n","    \n","    # adding a start and an end token to the sentence\n","    # so that the model know when to start and stop predicting.\n","    w = '<start> ' + w + ' <end>'\n","    return w"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"id":"nZ-OvGrnJHiA","colab_type":"code","outputId":"06f8b692-50da-4220-9f92-248dd64fd0f7","executionInfo":{"status":"ok","timestamp":1562229641004,"user_tz":-540,"elapsed":935,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":51}},"source":["en_sentence = u\"May I borrow this book?\"\n","sp_sentence = u\"¿Puedo tomar prestado este libro?\"\n","print(preprocess_sentence(en_sentence))\n","print(preprocess_sentence(sp_sentence).encode('utf-8'))"],"execution_count":0,"outputs":[{"output_type":"stream","text":["<start> may i borrow this book ? <end>\n","b'<start> \\xc2\\xbf puedo tomar prestado este libro ? <end>'\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"OHn4Dct23jEm","colab":{}},"source":["# 1. Remove the accents\n","# 2. Clean the sentences\n","# 3. Return word pairs in the format: [ENGLISH, SPANISH]\n","def create_dataset(path : str, num_examples : int) -> List[List[str]] :\n","    lines = open(path, encoding = 'UTF-8').read().strip().split('\\n')\n","    \n","    word_pairs = [\n","        [preprocess_sentence(w) for w in l.split('\\t')]\n","        for l in lines[:num_examples]\n","    ]\n","    \n","    return word_pairs"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"9xbqO7Iie9bb","colab":{}},"source":["# This class creates a word -> index mapping (e.g,. \"dad\" -> 5) and vice-versa \n","# (e.g., 5 -> \"dad\") for each language,\n","class LanguageIndex():\n","    def __init__(self, lang : Iterable[str]):\n","        self.lang = lang\n","        self.word2idx : Dict[str, int] = {}\n","        self.idx2word : Dict[int, str] = {}\n","        self.vocab = set()\n","    \n","        self.create_index()\n","    \n","    def create_index(self):\n","        for phrase in self.lang:\n","            self.vocab.update(phrase.split(' '))\n","    \n","        self.vocab = sorted(self.vocab)\n","    \n","        #self.word2idx['<pad>'] = 0\n","        #for index, word in enumerate(self.vocab):\n","        #    self.word2idx[word] = index + 1\n","        self.word2idx = { word : idx for idx, word in enumerate(self.vocab, 1) }\n","        self.word2idx['<pad>'] = 0\n","\n","        #for word, index in self.word2idx.items():\n","        #    self.idx2word[index] = word\n","        self.idx2word = { idx : word for word, idx in self.word2idx.items() }\n"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"eAY9k49G3jE_","colab":{}},"source":["def max_length(tensor):\n","    return max(len(t) for t in tensor)\n","\n","def load_dataset(path : str, num_examples : int):\n","    # creating cleaned input, output pairs\n","    pairs : List[List[str]] = create_dataset(path, num_examples)\n","\n","    # index language using the class defined above    \n","    inp_lang = LanguageIndex(sp for en, sp in pairs)\n","    targ_lang = LanguageIndex(en for en, sp in pairs)\n","    \n","    # Vectorize the input and target languages\n","    \n","    # Spanish sentences\n","    input_tensor : List[List[int]] = [\n","        [inp_lang.word2idx[s] for s in sp.split(' ')]\n","        for en, sp in pairs\n","    ]\n","    \n","    # English sentences\n","    target_tensor : List[List[int]] = [\n","        [targ_lang.word2idx[s] for s in en.split(' ')]\n","        for en, sp in pairs\n","    ]\n","    \n","    # Calculate max_length of input and output tensor\n","    # Here, we'll set those to the longest sentence in the dataset\n","    max_length_inp = max_length(input_tensor)\n","    max_length_tar = max_length(target_tensor)\n","    \n","    # Padding the input and output tensor to the maximum length\n","    input_tensor : np.ndarray = tf.keras.preprocessing.sequence.pad_sequences(\n","        input_tensor,\n","        maxlen = max_length_inp,\n","        padding = 'post')\n","    \n","    target_tensor : np.ndarray = tf.keras.preprocessing.sequence.pad_sequences(\n","        target_tensor,\n","        maxlen = max_length_tar,\n","        padding = 'post')\n","    \n","    return (\n","        input_tensor, target_tensor,\n","        inp_lang, targ_lang,\n","        max_length_inp, max_length_tar)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HXam20m8dZS3","colab_type":"text"},"source":["> [pad_sequences](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/preprocessing/sequence/pad_sequences)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"GOi42V79Ydlr"},"source":["### Limit the size of the dataset to experiment faster (optional)\n","\n","Training on the complete dataset of >100,000 sentences will take a long time. To train faster, we can limit the size of the dataset to 30,000 sentences (of course, translation quality degrades with less data):\n","\n","> 10万文を超える完全なデータセットのトレーニングには長い時間がかかります。より早く訓練するために、データセットのサイズを3万文に制限することができます（もちろん少ないデータでは翻訳品質が低下します）。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"cnxC7q-j3jFD","colab":{}},"source":["# Try experimenting with the size of that dataset\n","num_examples = 30000\n","(input_tensor, target_tensor, inp_lang, targ_lang,\n"," max_length_inp, max_length_targ) = load_dataset(path_to_file, num_examples)"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"4QILQkOs3jFG","outputId":"744e1d16-61c9-490f-ce0c-98cd74139a3f","executionInfo":{"status":"ok","timestamp":1562227548322,"user_tz":-540,"elapsed":476,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# Creating training and validation sets using an 80-20 split\n","(input_tensor_train, input_tensor_val,\n"," target_tensor_train, target_tensor_val) = train_test_split(\n","    input_tensor,\n","    target_tensor,\n","    test_size = 0.2)\n","\n","# Show length\n","(len(input_tensor_train), len(target_tensor_train),\n"," len(input_tensor_val), len(target_tensor_val))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["(24000, 24000, 6000, 6000)"]},"metadata":{"tags":[]},"execution_count":55}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"rgCLkfv5uO3d"},"source":["### Create a tf.data dataset"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"TqHsArVZ3jFS","colab":{}},"source":["BUFFER_SIZE = len(input_tensor_train)\n","BATCH_SIZE = 64\n","N_BATCH = BUFFER_SIZE//BATCH_SIZE\n","embedding_dim = 256\n","units = 1024\n","vocab_inp_size = len(inp_lang.word2idx)\n","vocab_tar_size = len(targ_lang.word2idx)\n","\n","dataset = tf.data.Dataset.from_tensor_slices(\n","    (input_tensor_train, target_tensor_train)).shuffle(BUFFER_SIZE)\n","dataset = dataset.batch(BATCH_SIZE, drop_remainder = True)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SOudl4Vq2I8u","colab_type":"text"},"source":["[from_tensor_slices](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/data/Dataset#from_tensor_slices)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"TNfHIF71ulLu"},"source":["## Write the encoder and decoder model\n","\n","Here, we'll implement an encoder-decoder model with attention which you can read about in the TensorFlow [Neural Machine Translation (seq2seq) tutorial](https://github.com/tensorflow/nmt). This example uses a more recent set of APIs. This notebook implements the [attention equations](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism) from the seq2seq tutorial. The following diagram shows that each input word is assigned a weight by the attention mechanism which is then used by the decoder to predict the next word in the sentence.\n","\n","> ここでは TensorFlow [Neural Machine Translation(seq2seq) チュートリアル](https://github.com/tensorflow/nmt)で読むことができる、Attention付きエンコーダー - デコーダーモデルを実装します。この例ではより新しい一連の API を使用しています。このノートブックは seq2seq チュートリアルから [Attention 計算式](https://github.com/tensorflow/nmt#background-on-the-attention-mechanism)を実装します。次の図は、各入力単語に Attention メカニズムによって重みが割り当てられていることを示しています。このメカニズムは文章内の次の単語を予測するためにデコーダーによって使用されます。\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_mechanism.jpg\" width=\"500\" alt=\"attention mechanism\">\n","\n","The input is put through an encoder model which gives us the encoder output of shape *(batch_size, max_length, hidden_size)* and the encoder hidden state of shape *(batch_size, hidden_size)*. \n","\n","> 入力はエンコーダーモデルを通過します。これにより形状が  *(batch_size, max_length, hidden_size)* のエンコーダー出力と、形状が *(batch_size, hidden_size)* のエンコーダー隠れ状態が得られます。\n","\n","Here are the equations that are implemented:\n","\n","> これが実装される計算式です：\n","\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_0.jpg\" alt=\"attention equation 0\" width=\"800\">\n","<img src=\"https://www.tensorflow.org/images/seq2seq/attention_equation_1.jpg\" alt=\"attention equation 1\" width=\"800\">\n","\n","We're using *Bahdanau attention*. Lets decide on notation before writing the simplified form:\n","\n","> 私たちは *Bahdanau attention* を使います。簡略形で書く前に表記法を決めましょう。\n","\n","* FC = Fully connected (dense) layer\n","* EO = Encoder output\n","* H = hidden state\n","* X = input to the decoder\n","\n","And the pseudo-code:\n","\n","> そして擬似コードです。\n","\n","* `score = FC(tanh(FC(EO) + FC(H)))`<br />\n","メモ：全結合層は入力と内部の重みの行列積を計算するレイヤのため、計算式中で $W_1$ や $v_a$ などの重みとの積の部分は `FC` により表現できる。ここで $h_t$ = `H`、$\\bar{h}_s$ = `EO` 。\n","* `attention weights = softmax(score, axis = 1)`<br />\n","Softmax by default is applied on the last axis but here we want to apply it on the *1st axis*, since the shape of score is *(batch_size, max_length, 1)*. `Max_length` is the length of our input. Since we are trying to assign a weight to each input, softmax should be applied on that axis.<br />\n","Softmax はデフォルトでは最後の軸に適用されますが、ここではスコアの形状が *(batch_size, max_length, 1)* であるため、*1番目の軸* に適用します。 `Max_length` は私達の入力の長さです。各入力に重みを割り当てようとしているので softmax をその軸に適用する必要があります。\n","* `context vector = sum(attention weights * EO, axis = 1)`<br />\n","Same reason as above for choosing axis as 1.<br />1番目の軸を選択したのは上と同じ理由です。\n","* `embedding output` = The input to the decoder X is passed through an embedding layer.<br />デコーダーへの入力 X は埋め込み層を通過します。\n","* `merged vector = concat(embedding output, context vector)`\n","* This merged vector is then given to the GRU<br />このマージされたベクトルは GRU に渡されます。\n","  \n","The shapes of all the vectors at each step have been specified in the comments in the code:\n","\n","> 各ステップのすべてのベクトルの形状はコード内のコメントで指定されています。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"avyJ_4VIUoHb","colab":{}},"source":["def gru(units):\n","    # If you have a GPU, we recommend using CuDNNGRU(provides a 3x speedup than GRU)\n","    # the code automatically does that.\n","    if tf.test.is_gpu_available() and False:\n","        # メモ：2019/07/04 tensorflow-gpu 2.0.0-beta1 に CuDNNGRUはない\n","        # return tf.keras.layers.CuDNNGRU(\n","        #    units,\n","        #    return_sequences = True,\n","        #    return_state = True,\n","        #    recurrent_initializer = 'glorot_uniform')\n","    else:\n","        return tf.keras.layers.GRU(\n","            units,\n","            return_sequences = True,\n","            return_state = True,\n","            recurrent_activation = 'sigmoid',\n","            recurrent_initializer = 'glorot_uniform')"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"nZ2rI24i3jFg","colab":{}},"source":["class Encoder(tf.keras.Model):\n","    def __init__(\n","        self,\n","        vocab_size : int,  # 語彙数\n","        embedding_dim : int,  # 埋め込みレイヤで出力する密ベクトルの次元数\n","        enc_units : int,  # Encoder(のGRU）の出力次元数\n","        batch_sz : int) :  # バッチサイズ\n","\n","        super(Encoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.enc_units = enc_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.enc_units)\n","\n","    # 入力 x は int の ndarray を convert_to_tensor() したもの\n","    # 型としては tensorflow.python.framework.ops.EagerTensor …\n","    # hidden は初回は initialze_hidden_state で返却したものが渡ってくる\n","    def call(self, x, hidden):\n","        x = self.embedding(x)\n","        output, state = self.gru(x, initial_state = hidden)        \n","        return output, state\n","    \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.enc_units))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eZeCPb-zC2Rt","colab_type":"text"},"source":["> [Embedding](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/Embedding)\n","> <br />\n","> <br />Embedding レイヤは入力の整数値（単語ID）を固定長の密ベクトルに変換する。第1引数 input_dim は語彙数。第2引数 output_dim は出力する密ベクトルの次元数。\n","\n","> [GRU](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/layers/GRU)\n","> <br />\n","> <br />※GPUが使われる条件について抜粋※\n","> <br />利用可能なランタイムハードウェアと制約に基づいて、このレイヤーはパフォーマンス最大化のため異なる実装（cuDNNベースまたは純粋なTensorFlow）を選択します。 GPUが利用可能で、レイヤーに対するすべての引数がCuDNNカーネルの要件を満たす場合（詳細は下記を参照）、高速なcuDNN実装を使用します。\n","> <br />\n","> <br />cuDNN実装を使用するための要件は次のとおりです。\n","> <br />\n","> 1. activation == 'tanh'\n","> 1. recurrent_activation == 'sigmoid'\n","> 1. recurrent_dropout == 0\n","> 1. unroll is False\n","> 1. use_bias is True\n","> 1. reset_after is True\n","> 1. 入力がマスクされていないか、厳密に右詰めされている\n","> <br />メモ：1 ～ 6 は既定値のため、この入力値の条件さえ満たせば GPU が使われる"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"yJ_B3mhW3jFk","colab":{}},"source":["class Decoder(tf.keras.Model):\n","    def __init__(\n","        self,\n","        vocab_size : int,  # 語彙数\n","        embedding_dim : int,  # 埋め込みレイヤで出力する密ベクトルの次元数\n","        dec_units : int,  # GRU の出力次元数\n","        batch_sz : int) :  # バッチサイズ\n","\n","        super(Decoder, self).__init__()\n","        self.batch_sz = batch_sz\n","        self.dec_units = dec_units\n","        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n","        self.gru = gru(self.dec_units)\n","        self.fc = tf.keras.layers.Dense(vocab_size)\n","        \n","        # used for attention\n","        self.W1 = tf.keras.layers.Dense(self.dec_units)\n","        self.W2 = tf.keras.layers.Dense(self.dec_units)\n","        self.V = tf.keras.layers.Dense(1)\n","        \n","    def call(self, x, hidden, enc_output):\n","        # enc_output shape == (batch_size, max_length, hidden_size)\n","        \n","        # hidden shape == (batch_size, hidden size)\n","        # hidden_with_time_axis shape == (batch_size, 1, hidden size)\n","        # we are doing this to perform addition to calculate the score\n","        hidden_with_time_axis = tf.expand_dims(hidden, 1)\n","        \n","        # score shape == (batch_size, max_length, 1)\n","        # we get 1 at the last axis\n","        # because we are applying tanh(FC(EO) + FC(H)) to self.V\n","        score = self.V(tf.nn.tanh(\n","            self.W1(enc_output) + self.W2(hidden_with_time_axis)))\n","        \n","        # attention_weights shape == (batch_size, max_length, 1)\n","        attention_weights = tf.nn.softmax(score, axis=1)\n","        \n","        # context_vector shape after sum == (batch_size, hidden_size)\n","        context_vector = attention_weights * enc_output\n","        context_vector = tf.reduce_sum(context_vector, axis = 1)\n","        \n","        # x shape after passing through embedding \n","        # == (batch_size, 1, embedding_dim)\n","        x = self.embedding(x)\n","        \n","        # x shape after concatenation\n","        # == (batch_size, 1, embedding_dim + hidden_size)\n","        x = tf.concat([tf.expand_dims(context_vector, 1), x], axis = -1)\n","        \n","        # passing the concatenated vector to the GRU\n","        output, state = self.gru(x)\n","        \n","        # output shape == (batch_size * 1, hidden_size)\n","        output = tf.reshape(output, (-1, output.shape[2]))\n","        \n","        # output shape == (batch_size * 1, vocab)\n","        x = self.fc(output)\n","        \n","        return x, state, attention_weights\n","        \n","    def initialize_hidden_state(self):\n","        return tf.zeros((self.batch_sz, self.dec_units))"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"P5UY8wko3jFp","colab":{}},"source":["encoder = Encoder(vocab_inp_size, embedding_dim, units, BATCH_SIZE)\n","decoder = Decoder(vocab_tar_size, embedding_dim, units, BATCH_SIZE)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"_ch_71VbIRfK"},"source":["## Define the optimizer and the loss function"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WmTHr5iV3jFr","colab":{}},"source":["optimizer = tf.keras.optimizers.Adam()\n","\n","def loss_function(real, pred):\n","    mask = 1 - np.equal(real, 0)\n","    loss_ = tf.nn.sparse_softmax_cross_entropy_with_logits(\n","        labels = real,\n","        logits = pred) * mask\n","    return tf.reduce_mean(loss_)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CrO56RpJ3OV_","colab_type":"text"},"source":["> [Adam](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/keras/optimizers/Adam)\n","> <br />\n","> [sparse_softmax_cross_entropy_with_logits](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/nn/sparse_softmax_cross_entropy_with_logits)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"DMVWzzsfNl4e"},"source":["## Checkpoints (Object-based saving)"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"Zj8bXQTgNwrF","colab":{}},"source":["checkpoint_dir = './training_checkpoints'\n","checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n","checkpoint = tf.train.Checkpoint(\n","    optimizer = optimizer,\n","    encoder = encoder,\n","    decoder = decoder)"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ubi7sTRv3Tey","colab_type":"text"},"source":["> [Checkpoint](https://www.tensorflow.org/versions/r2.0/api_docs/python/tf/train/Checkpoint)"]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"hpObfY22IddU"},"source":["## Training\n","\n","1. Pass the *input* through the *encoder* which return *encoder output* and the *encoder hidden state*.\n","<br />出力と隠れ状態を返すエンコーダーに入力を渡します。\n","2. The encoder output, encoder hidden state and the decoder input (which is the *start token*) is passed to the decoder.\n","<br />エンコーダーの出力と隠れ状態、およびデコーダー入力（開始トークン）がデコーダに渡されます。\n","3. The decoder returns the *predictions* and the *decoder hidden state*.\n","<br />デコーダーは予測結果とデコーダー隠れ状態を返します。\n","4. The decoder hidden state is then passed back into the model and the predictions are used to calculate the loss.\n","<br />デコーダー隠れ状態がモデルに戻され、予測結果が損失の計算に使用されます。\n","5. Use *teacher forcing* to decide the next input to the decoder.\n","<br />デコーダーへの次の入力を決定するために *teacher forcing* を使用します。\n","6. *Teacher forcing* is the technique where the *target word* is passed as the *next input* to the decoder.\n","<br />*Teacher forcing* は目標（正解）の単語を次の入力としてデコーダーに渡す手法です。\n","7. The final step is to calculate the gradients and apply it to the optimizer and backpropagate.\n","<br />最後のステップは勾配を計算し、それをオプティマイザに適用して逆伝播することです。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"ddefjBMa3jF0","outputId":"5e343076-9f2d-44f9-f778-0ac72a9a974f","executionInfo":{"status":"ok","timestamp":1562228460964,"user_tz":-540,"elapsed":894270,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":1000}},"source":["EPOCHS = 10\n","\n","for epoch in range(EPOCHS):\n","    start = time.time()\n","\n","    hidden = encoder.initialize_hidden_state()\n","    total_loss = 0\n","\n","    for (batch, (inp, targ)) in enumerate(dataset):\n","        loss = 0\n","\n","        with tf.GradientTape() as tape:\n","            enc_output, enc_hidden = encoder(inp, hidden)\n","            dec_hidden = enc_hidden\n","            dec_input = tf.expand_dims(\n","                [targ_lang.word2idx['<start>']] * BATCH_SIZE, 1)\n","\n","            # Teacher forcing - feeding the target as the next input\n","            for t in range(1, targ.shape[1]):\n","                # passing enc_output to the decoder\n","                predictions, dec_hidden, _ = decoder(\n","                    dec_input,\n","                    dec_hidden,\n","                    enc_output)\n","\n","                loss += loss_function(targ[:, t], predictions)\n","\n","                # using teacher forcing\n","                dec_input = tf.expand_dims(targ[:, t], 1)\n","\n","        batch_loss = (loss / int(targ.shape[1]))\n","        total_loss += batch_loss\n","        variables = encoder.variables + decoder.variables\n","        gradients = tape.gradient(loss, variables)\n","        \n","        optimizer.apply_gradients(zip(gradients, variables))\n","        \n","        if batch % 100 == 0:\n","            print(f'Epoch { epoch + 1 } Batch { batch } '\n","                  f'Loss { batch_loss.numpy() :.4f}')\n","\n","    # saving (checkpoint) the model every 2 epochs\n","    if (epoch + 1) % 2 == 0:\n","        checkpoint.save(file_prefix = checkpoint_prefix)\n","    \n","    print(f'Epoch {epoch + 1} Loss {total_loss / N_BATCH:.4f}')\n","    print(f'Time taken for 1 epoch { time.time() - start } sec\\n')"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1 Batch 0 Loss 4.6996\n","Epoch 1 Batch 100 Loss 2.1855\n","Epoch 1 Batch 200 Loss 1.8083\n","Epoch 1 Batch 300 Loss 1.7019\n","Epoch 1 Loss 2.0081\n","Time taken for 1 epoch 89.5616569519043 sec\n","\n","Epoch 2 Batch 0 Loss 1.6191\n","Epoch 2 Batch 100 Loss 1.4720\n","Epoch 2 Batch 200 Loss 1.3713\n","Epoch 2 Batch 300 Loss 1.3228\n","Epoch 2 Loss 1.3849\n","Time taken for 1 epoch 89.85656118392944 sec\n","\n","Epoch 3 Batch 0 Loss 1.1686\n","Epoch 3 Batch 100 Loss 1.0518\n","Epoch 3 Batch 200 Loss 1.0033\n","Epoch 3 Batch 300 Loss 0.9391\n","Epoch 3 Loss 1.0059\n","Time taken for 1 epoch 88.45564079284668 sec\n","\n","Epoch 4 Batch 0 Loss 0.8311\n","Epoch 4 Batch 100 Loss 0.7297\n","Epoch 4 Batch 200 Loss 0.6649\n","Epoch 4 Batch 300 Loss 0.6592\n","Epoch 4 Loss 0.6844\n","Time taken for 1 epoch 89.5451307296753 sec\n","\n","Epoch 5 Batch 0 Loss 0.5639\n","Epoch 5 Batch 100 Loss 0.4717\n","Epoch 5 Batch 200 Loss 0.4463\n","Epoch 5 Batch 300 Loss 0.3966\n","Epoch 5 Loss 0.4504\n","Time taken for 1 epoch 88.97400498390198 sec\n","\n","Epoch 6 Batch 0 Loss 0.3566\n","Epoch 6 Batch 100 Loss 0.3094\n","Epoch 6 Batch 200 Loss 0.3085\n","Epoch 6 Batch 300 Loss 0.2577\n","Epoch 6 Loss 0.2971\n","Time taken for 1 epoch 89.44582605361938 sec\n","\n","Epoch 7 Batch 0 Loss 0.2484\n","Epoch 7 Batch 100 Loss 0.2121\n","Epoch 7 Batch 200 Loss 0.1979\n","Epoch 7 Batch 300 Loss 0.1839\n","Epoch 7 Loss 0.2035\n","Time taken for 1 epoch 88.82627654075623 sec\n","\n","Epoch 8 Batch 0 Loss 0.1674\n","Epoch 8 Batch 100 Loss 0.1295\n","Epoch 8 Batch 200 Loss 0.1548\n","Epoch 8 Batch 300 Loss 0.1392\n","Epoch 8 Loss 0.1456\n","Time taken for 1 epoch 89.98136878013611 sec\n","\n","Epoch 9 Batch 0 Loss 0.1218\n","Epoch 9 Batch 100 Loss 0.1096\n","Epoch 9 Batch 200 Loss 0.1077\n","Epoch 9 Batch 300 Loss 0.0927\n","Epoch 9 Loss 0.1089\n","Time taken for 1 epoch 89.16745162010193 sec\n","\n","Epoch 10 Batch 0 Loss 0.0878\n","Epoch 10 Batch 100 Loss 0.0890\n","Epoch 10 Batch 200 Loss 0.0825\n","Epoch 10 Batch 300 Loss 0.0878\n","Epoch 10 Loss 0.0841\n","Time taken for 1 epoch 89.50136518478394 sec\n","\n"],"name":"stdout"}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"mU3Ce8M6I3rz"},"source":["## Translate\n","\n","* The evaluate function is similar to the training loop, except we don't use *teacher forcing* here. The input to the decoder at each time step is its previous predictions along with the hidden state and the encoder output.\n","<br />評価関数は *teacher forcing* を使用しないことを除いてトレーニングループと似ています。各時間ステップのデコーダーへの入力は、1つ前の予測値および隠れ状態とエンコーダー出力です。\n","* Stop predicting when the model predicts the *end token*.\n","<br />モデルが終了トークンを予測したら予測を停止します。\n","* And store the *attention weights for every time step*.\n","<br />そしてタイムステップごとに attention weights を保存します。\n","\n","Note: The encoder output is calculated only once for one input.\n","\n","> 注：エンコーダ出力は1つの入力に対して1回だけ計算されます。"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"EbQpyYs13jF_","colab":{}},"source":["def evaluate(\n","    sentence,\n","    encoder,\n","    decoder,\n","    inp_lang,\n","    targ_lang,\n","    max_length_inp,\n","    max_length_targ):\n","\n","    attention_plot = np.zeros((max_length_targ, max_length_inp))\n","    \n","    sentence = preprocess_sentence(sentence)\n","\n","    inputs = [inp_lang.word2idx[i] for i in sentence.split(' ')]\n","    inputs = tf.keras.preprocessing.sequence.pad_sequences(\n","        [inputs],\n","        maxlen = max_length_inp,\n","        padding = 'post')\n","    inputs = tf.convert_to_tensor(inputs)\n","    \n","    result = ''\n","\n","    hidden = [tf.zeros((1, units))]\n","    enc_out, enc_hidden = encoder(inputs, hidden)\n","\n","    dec_hidden = enc_hidden\n","    dec_input = tf.expand_dims([targ_lang.word2idx['<start>']], 0)\n","\n","    for t in range(max_length_targ):\n","        predictions, dec_hidden, attention_weights = decoder(\n","            dec_input,\n","            dec_hidden,\n","            enc_out)\n","        \n","        # storing the attention weights to plot later on\n","        attention_weights = tf.reshape(attention_weights, (-1, ))\n","        attention_plot[t] = attention_weights.numpy()\n","\n","        predicted_id = tf.argmax(predictions[0]).numpy()\n","\n","        result += targ_lang.idx2word[predicted_id] + ' '\n","\n","        if targ_lang.idx2word[predicted_id] == '<end>':\n","            return result, sentence, attention_plot\n","        \n","        # the predicted ID is fed back into the model\n","        dec_input = tf.expand_dims([predicted_id], 0)\n","\n","    return result, sentence, attention_plot"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"s5hQWlbN3jGF","colab":{}},"source":["# function for plotting the attention weights\n","def plot_attention(attention, sentence, predicted_sentence):\n","    fig = plt.figure(figsize = (5, 5))\n","    ax = fig.add_subplot(1, 1, 1)\n","    ax.matshow(attention, cmap = 'viridis')\n","    \n","    fontdict = {'fontsize' : 10}\n","    \n","    ax.set_xticklabels([''] + sentence, fontdict = fontdict, rotation = 90)\n","    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n","\n","    plt.show()"],"execution_count":0,"outputs":[]},{"cell_type":"code","metadata":{"colab_type":"code","id":"sl9zUHzg3jGI","colab":{}},"source":["def translate(\n","    sentence,\n","    encoder,\n","    decoder,\n","    inp_lang,\n","    targ_lang,\n","    max_length_inp,\n","    max_length_targ):\n","\n","    result, sentence, attention_plot = evaluate(\n","        sentence,\n","        encoder,\n","        decoder,\n","        inp_lang,\n","        targ_lang,\n","        max_length_inp,\n","        max_length_targ)\n","        \n","    print('Input: {}'.format(sentence))\n","    print('Predicted translation: {}'.format(result))\n","    \n","    attention_plot = attention_plot[\n","        :len(result.split(' ')), :len(sentence.split(' '))]\n","    plot_attention(attention_plot, sentence.split(' '), result.split(' '))"],"execution_count":0,"outputs":[]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"n250XbnjOaqP"},"source":["## Restore the latest checkpoint and test"]},{"cell_type":"code","metadata":{"colab_type":"code","id":"UJpT9D5_OgP6","outputId":"6bc30886-c7c7-4f6c-e205-8bd57c2685c9","executionInfo":{"status":"ok","timestamp":1562229092577,"user_tz":-540,"elapsed":1236,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":34}},"source":["# restoring the latest checkpoint in checkpoint_dir\n","checkpoint.restore(tf.train.latest_checkpoint(checkpoint_dir))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7fd09f4fb240>"]},"metadata":{"tags":[]},"execution_count":68}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"WrAM0FDomq3E","outputId":"4ab1d99b-b843-4e36-d855-dd5fa1671ca1","executionInfo":{"status":"ok","timestamp":1562229212354,"user_tz":-540,"elapsed":942,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":387}},"source":["translate(u'hace mucho frio aqui.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: <start> hace mucho frio aqui . <end>\n","Predicted translation: it s too cold here . <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAASoAAAFQCAYAAAAIgzOIAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"zSx2iM36EZQZ","outputId":"5fcedc5b-f790-4aa7-c9ab-7635bf423a08","executionInfo":{"status":"ok","timestamp":1562229221059,"user_tz":-540,"elapsed":902,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":387}},"source":["translate(u'esta es mi vida.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: <start> esta es mi vida . <end>\n","Predicted translation: this is my life . <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAUwAAAFQCAYAAADUeke/AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"A3LLCx3ZE0Ls","outputId":"412d9589-75cf-40f9-b77e-7f9d38906b52","executionInfo":{"status":"ok","timestamp":1562229226654,"user_tz":-540,"elapsed":775,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":354}},"source":["translate(u'todavia estan en casa?', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: <start> todavia estan en casa ? <end>\n","Predicted translation: are still home ? <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAVMAAAEvCAYAAAD4qyLOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\npAYsU0lqwDKVpAYsU0lqwDKVpAb+P7d613tVGl1RAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"code","metadata":{"colab_type":"code","id":"DUQVLVqUE1YW","outputId":"d229d82a-2876-4c2b-e617-72509d85b5da","executionInfo":{"status":"ok","timestamp":1562229230206,"user_tz":-540,"elapsed":757,"user":{"displayName":"y k","photoUrl":"","userId":"10615241839476507546"}},"colab":{"base_uri":"https://localhost:8080/","height":404}},"source":["# wrong translation\n","translate(u'trata de averiguarlo.', encoder, decoder, inp_lang, targ_lang, max_length_inp, max_length_targ)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Input: <start> trata de averiguarlo . <end>\n","Predicted translation: try to figure it . <end> \n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAASUAAAFhCAYAAAA2rrwNAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{"tags":[]}}]},{"cell_type":"markdown","metadata":{"colab_type":"text","id":"RTe5P5ioMJwN"},"source":["## Next steps\n","\n","* [Download a different dataset](http://www.manythings.org/anki/) to experiment with translations, for example, English to German, or English to French.\n","<br />[別のデータセットをダウンロード](http://www.manythings.org/anki/)して、たとえば英語からドイツ語、または英語からフランス語への翻訳を試してください。\n","* Experiment with training on a larger dataset, or using more epochs\n","<br />より大きなデータセットでのトレーニング、またはより多くのエポックの使用を試してください。"]}]}